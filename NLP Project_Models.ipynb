{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb978f41",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4e4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support,balanced_accuracy_score\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from nltk.tokenize import MWETokenizer,word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "path = r\"C:\\YZC\\NUS\\Semester 2\\CS4248 Natural Language Processing\\Project\\scicite\"\n",
    "os.chdir(path)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb153f",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc107df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>citeEnd</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>citeStart</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>isKeyCitation</th>\n",
       "      <th>id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>excerpt_index</th>\n",
       "      <th>label2</th>\n",
       "      <th>label2_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explicit</td>\n",
       "      <td>175.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>168.0</td>\n",
       "      <td>However, how frataxin interacts with the Fe-S ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649</td>\n",
       "      <td>894be9b4ea46a5c422e81ef3c241072d4c73fdc0</td>\n",
       "      <td>True</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>explicit</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Novel Quantitative Trait Loci for Seminal Root...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>In the study by Hickey et al. (2012), spikes w...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b</td>\n",
       "      <td>b6642e19efb8db5623b3cc4eef1c5822a6151107</td>\n",
       "      <td>True</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explicit</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>225.0</td>\n",
       "      <td>The drug also reduces catecholamine secretion,...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc</td>\n",
       "      <td>4e6a17fb8d7a3cada601d942e22eb5da6d01adbd</td>\n",
       "      <td>False</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>explicit</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>46.0</td>\n",
       "      <td>By clustering with lowly aggressive close kin ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b</td>\n",
       "      <td>2cc6ff899bf17666ad35893524a4d61624555ed7</td>\n",
       "      <td>False</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explicit</td>\n",
       "      <td>239.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>234.0</td>\n",
       "      <td>Ophthalmic symptoms are rare manifestations of...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175</td>\n",
       "      <td>a5bb0ff1a026944d2a47a155462959af2b8505a8</td>\n",
       "      <td>False</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source  citeEnd                                        sectionName  \\\n",
       "0  explicit    175.0                                       Introduction   \n",
       "1  explicit     36.0  Novel Quantitative Trait Loci for Seminal Root...   \n",
       "2  explicit    228.0                                       Introduction   \n",
       "3  explicit    110.0                                         Discussion   \n",
       "4  explicit    239.0                                         Discussion   \n",
       "\n",
       "   citeStart                                             string       label  \\\n",
       "0      168.0  However, how frataxin interacts with the Fe-S ...  background   \n",
       "1       16.0  In the study by Hickey et al. (2012), spikes w...  background   \n",
       "2      225.0  The drug also reduces catecholamine secretion,...  background   \n",
       "3       46.0  By clustering with lowly aggressive close kin ...  background   \n",
       "4      234.0  Ophthalmic symptoms are rare manifestations of...  background   \n",
       "\n",
       "   label_confidence                             citingPaperId  \\\n",
       "0               1.0  1872080baa7d30ec8fb87be9a65358cd3a7fb649   \n",
       "1               1.0  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b   \n",
       "2               1.0  9cdf605beb1aa1078f235c4332b3024daa8b31dc   \n",
       "3               1.0  d9f3207db0c79a3b154f3875c9760cc6b056904b   \n",
       "4               1.0  88b86556857f4374842d2af2e359576806239175   \n",
       "\n",
       "                               citedPaperId  isKeyCitation  \\\n",
       "0  894be9b4ea46a5c422e81ef3c241072d4c73fdc0           True   \n",
       "1  b6642e19efb8db5623b3cc4eef1c5822a6151107           True   \n",
       "2  4e6a17fb8d7a3cada601d942e22eb5da6d01adbd          False   \n",
       "3  2cc6ff899bf17666ad35893524a4d61624555ed7          False   \n",
       "4  a5bb0ff1a026944d2a47a155462959af2b8505a8          False   \n",
       "\n",
       "                                                  id  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...   \n",
       "\n",
       "                                           unique_id  excerpt_index label2  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...             11    NaN   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...              2    NaN   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...              0    NaN   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...              3    NaN   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...              2    NaN   \n",
       "\n",
       "   label2_confidence  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('train.jsonl',lines=True)\n",
    "df_test = pd.read_json('test.jsonl',lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Questions:\n",
    "excerpt_index?\n",
    "citeStart, citeEnd?\n",
    "label2 (supportiveness)?\n",
    "isKeyCitation?\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e3370",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83433209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_test():\n",
    "    null_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any(): null_cols.append(col)\n",
    "    print(null_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de41c53",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f712d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = MWETokenizer([('<','bos','>'),('<','eos','>')],separator='')\n",
    "# tk.add_mwe([('<','bos','>'),('<','eos','>')])\n",
    "\n",
    "def convert_label(value):\n",
    "    if value == 'background':\n",
    "        label = 0\n",
    "    elif value == 'method':\n",
    "        label = 1\n",
    "    elif value == 'result':\n",
    "        label = 2\n",
    "    return label\n",
    "\n",
    "def NA_impute(df):\n",
    "    df['label2'] = df['label2'].fillna('cant_determine')\n",
    "    try:\n",
    "        print(len(df).df.columns)\n",
    "        df = df.drop(columns='label2_confidence',axis=1)\n",
    "        df_type = 'train'\n",
    "    except:\n",
    "        df_type = 'test'\n",
    "    df['label_confidence'] = df['label_confidence'].fillna(df['label_confidence'].mean())\n",
    "    df['citeStart'] = df['citeStart'].fillna(df['citeStart'].mean().astype(np.int64))\n",
    "    df['citeEnd'] = df['citeEnd'].fillna(df['citeEnd'].mean().astype(np.int64))\n",
    "    df['source'] = df['source'].fillna('unknown')\n",
    "    df['sectionName'] = df['sectionName'].fillna('unknown')\n",
    "    return df, df_type\n",
    "\n",
    "def process_df(df):\n",
    "    df, df_type = NA_impute(df)\n",
    "    for col in ['citeStart','citeEnd']:\n",
    "        df[col] = df[col].astype('int64')\n",
    "    feature_cols = ['source', 'citeEnd', 'sectionName', 'citeStart', 'label_confidence', 'citingPaperId', 'citedPaperId', 'isKeyCitation', 'excerpt_index', 'label2', 'label2_confidence']\n",
    "    if df_type == 'test':\n",
    "        feature_cols.remove('label2_confidence')\n",
    "    df['edited_string'] = ''\n",
    "    for col in feature_cols:\n",
    "        df['edited_string'] += col + ': ' + df[col].astype(str) + '[SEP]'\n",
    "    df['edited_string'] += df['string']\n",
    "    df['tagged_string'] = '<BOS>' + df['string'] + '<EOS>'\n",
    "    df['label_num'] = df['label'].apply(lambda x: convert_label(x))\n",
    "    df['string_lower'] = df['tagged_string'].apply(lambda x: x.lower())\n",
    "    df['tokens_lower'] = df['string_lower'].apply(lambda x: tk.tokenize(word_tokenize(x)))  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a28734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = process_df(df)\n",
    "df_test = process_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00abfd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers - KIV\n",
    "df_train = df_train.loc[df_train['tokens_lower'].str.len() <= 100]\n",
    "df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7a58a",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38280d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hypotheses:\n",
    "Word embeddings\n",
    "Combine word embeddings with features\n",
    "Attention mechanism\n",
    "Include the sentences immediately before and after\n",
    "Activation functions\n",
    "\n",
    "Open questions\n",
    "- Where can we get more contextual information? How to incorporate?\n",
    "- variables:\n",
    "    - lower case? stopwords? lemmatization? any words to include?\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cf439",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate word_embeddings (ok)\n",
    "Convert labels (ok)\n",
    "Generate other features?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762ebf8",
   "metadata": {},
   "source": [
    "#### Preparing Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4319d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input array (batch_first = True): N (batch size) * L (sequence length) * H_in (input size)\n",
    "'''\n",
    "def generate_X(df,token_col,model,seq_len,vec_size):\n",
    "    '''\n",
    "    token_col: col for tokens in dataframe\n",
    "    model: word vectorization model\n",
    "    '''\n",
    "    X = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        i_arr = [model.wv[token] for token in df.at[i,token_col]]\n",
    "        if len(i_arr) < seq_len:\n",
    "            while len(i_arr) < seq_len:\n",
    "                i_arr.append(np.zeros(vec_size))\n",
    "        elif len(i_arr) > seq_len:\n",
    "            i_arr = i_arr[:seq_len]\n",
    "        X.append(i_arr)\n",
    "    return np.array(X)\n",
    "\n",
    "def convert_y(y): # for training and validation labels. For y_val, keep one multi-target and one single-target version\n",
    "    y_nn = np.zeros([y.shape[0],3])\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            y_nn[i,0] = 1\n",
    "        elif y[i] == 1:\n",
    "            y_nn[i,1] = 1\n",
    "        elif y[i] == 2: \n",
    "            y_nn[i,2] = 1\n",
    "    return y_nn\n",
    "\n",
    "def to_tensor(arr,dtype='Float'):\n",
    "    if dtype == \"Float\":\n",
    "        arr = torch.from_numpy(arr).type(torch.FloatTensor)\n",
    "    elif dtype == \"Long\":\n",
    "        arr = torch.from_numpy(arr).type(torch.LongTensor)\n",
    "    return arr\n",
    "\n",
    "def check_dist_y(y,n=None):\n",
    "    '''\n",
    "    y: torch array\n",
    "    n: last index of sample\n",
    "    '''\n",
    "    if n == None:\n",
    "        print(torch.sum(y_train,dim=0))\n",
    "    else:\n",
    "        print(torch.sum(y_train[:n],dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e9188c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 200 # hp\n",
    "seq_length = 100 # max word length based on tokenization, including padding tokens. Outliers removed (~ 1.5% of dataset)\n",
    "corpus = df_train['tokens_lower'].tolist()\n",
    "ft = FastText(corpus,vector_size=vec_size,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd156264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef5e3f232bf468ca4efea577b013d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47bfd42b6d64769a79209bf464b4e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_core = generate_X(df_train,'tokens_lower',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_test = generate_X(df_test,'tokens_lower',\n",
    "                   ft,seq_length,vec_size)\n",
    "y_core = df_train['label_num'].to_numpy()\n",
    "y_test = df_test['label_num'].to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_core,y_core,test_size=0.2,random_state= 1)\n",
    "y_train, y_val_nn = convert_y(y_train),convert_y(y_val)\n",
    "X_train, X_val, X_test, y_train, y_val_nn = to_tensor(X_train), to_tensor(X_val), to_tensor(X_test), to_tensor(y_train), to_tensor(y_val_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c9a7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([X_train,X_val,X_test,y_train,y_val_nn],'data_arrays_v0.pt') # embeddings only\n",
    "np.savez('y_arr_v0.npz',y_val=y_val,y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8846f0",
   "metadata": {},
   "source": [
    "#### **[Note] Start from this Cell for Subsequent Runs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support,balanced_accuracy_score\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from nltk.tokenize import MWETokenizer,word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "path = r\"C:\\YZC\\NUS\\Semester 2\\CS4248 Natural Language Processing\\Project\\scicite\"\n",
    "os.chdir(path)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc06333",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,X_test,y_train,y_val_nn = torch.load('data_arrays_v0.pt')\n",
    "data = np.load('y_arr_v0.npz')\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "vec_size = 300 # hp\n",
    "seq_length = 100 # max word length based on tokenization, including padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b9262e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist(arr):\n",
    "    unique, counts = np.unique(arr,return_counts=True)\n",
    "    return list(zip(unique,counts))\n",
    "    \n",
    "def resample_arr(X,y,rng):\n",
    "    y_0 = y[:,0] == 1 # class 0\n",
    "    y_non0 =  y[:,0] == 0 # class 1 or 2\n",
    "    idx_0 = y_0.nonzero().numpy() # list of lists (numpy)\n",
    "    idx_non0 = y_non0.nonzero() # list of lists (torch)\n",
    "    idx_sample_0 = rng.choice(idx_0,len(idx_0)//3) # as size of class 0 ~ 4x that of other classes\n",
    "    idx_sample_0 = torch.from_numpy(idx_sample_0).type(torch.LongTensor)\n",
    "    idx_sample = torch.cat((idx_sample_0.squeeze(1),idx_non0.squeeze(1))) \n",
    "    del y_0,y_non0,idx_0,idx_non0,idx_sample_0\n",
    "    return X[idx_sample], y[idx_sample]\n",
    "\n",
    "def train_and_save(model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   X_val,y_val_nn,y_val,n_epochs=40):\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "        # Data resampling to offset class imbalance\n",
    "#         X_sample, y_sample = resample_arr(X_train,y_train,rng)\n",
    "#         check_dist(y_sample.numpy()[:,0]);check_dist(y_sample.numpy()[:,1]); check_dist(y_sample.numpy()[:,2]) # Troubleshooting\n",
    "#         loader = DataLoader(TensorDataset(X_sample.to(device),y_sample.to(device)),batch_size=batch_size)\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred_batch = model(X_batch)\n",
    "            loss = loss_fn(y_pred_batch,y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_nn = model(X_val)\n",
    "            y_pred = torch.argmax(y_pred_nn,1).detach().cpu()\n",
    "            y_dist = check_dist(y_pred) # Troubleshooting \n",
    "            print(y_dist) # Troubleshooting\n",
    "            bal_accuracy = balanced_accuracy_score(y_val,y_pred)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                precision, recall, fscore, support = precision_recall_fscore_support(y_val,y_pred,average='weighted')\n",
    "            val_loss = loss_fn(y_pred_nn,y_val_nn)\n",
    "            tqdm.write(f\"Epoch{epoch+1}: val loss={val_loss}, balanced accuracy={bal_accuracy}, precision={precision}, recall={recall}, fscore={fscore}\")\n",
    "            \n",
    "            # Early Stopping\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_epoch = epoch + 1\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                patience = 10\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print(f'Early stopping triggered at epoch {epoch+1}. Best model is from epoch {best_epoch}.')\n",
    "                    break\n",
    "        \n",
    "        # For memory saving\n",
    "        try:\n",
    "            del X_sample,y_sample\n",
    "            gc.collect()\n",
    "        except:\n",
    "            True\n",
    "    torch.save(best_model_weights,filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737358e0",
   "metadata": {},
   "source": [
    "#### Naive RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abc540fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "class RNN_base(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN_base,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "        self.ff = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1,x.size(0),self.hidden_size).to(device)\n",
    "        # 1: number of RNN layers. x.size(0): batch_size.\n",
    "        _, hidden = self.rnn(x,h0)\n",
    "        output = self.ff(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6aa8dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5898660095487448 0.2782997073771754 0.13183428307407977\n"
     ]
    }
   ],
   "source": [
    "# For reference\n",
    "class0 = check_dist(y_train[:,0])[1][1]\n",
    "class1 = check_dist(y_train[:,1])[1][1]\n",
    "class2 = check_dist(y_train[:,2])[1][1]\n",
    "print(class0/len(y_train),class1/len(y_train),class2/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60aa9834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cb639739044dbd920fdea708d003d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1624)]\n",
      "Epoch1: val loss=1.5997633934020996, balanced accuracy=0.3333333333333333, precision=0.34363852556480373, recall=0.5862068965517241, fscore=0.43328335832083953\n",
      "[(0, 777), (1, 847)]\n",
      "Epoch2: val loss=1.602116346359253, balanced accuracy=0.34521586412342714, precision=0.43121543866057965, recall=0.43657635467980294, fscore=0.41947899857185345\n",
      "[(0, 807), (1, 817)]\n",
      "Epoch3: val loss=1.6013455390930176, balanced accuracy=0.34628295762749545, precision=0.4304754223676248, recall=0.44273399014778325, fscore=0.42452083498060317\n",
      "[(0, 815), (1, 809)]\n",
      "Epoch4: val loss=1.6110438108444214, balanced accuracy=0.3436652438753279, precision=0.4260496404030917, recall=0.44027093596059114, fscore=0.4217310852213961\n",
      "[(0, 809), (1, 814), (2, 1)]\n",
      "Epoch5: val loss=1.6050264835357666, balanced accuracy=0.34215352807789784, precision=0.4274954661020898, recall=0.43903940886699505, fscore=0.4215638727667801\n",
      "[(0, 1623), (2, 1)]\n",
      "Epoch6: val loss=1.6029331684112549, balanced accuracy=0.3333333333333333, precision=0.3438502560180169, recall=0.5862068965517241, fscore=0.43345162370271173\n",
      "[(0, 817), (1, 806), (2, 1)]\n",
      "Epoch7: val loss=1.6054810285568237, balanced accuracy=0.3423925125605798, precision=0.427722930488311, recall=0.4408866995073892, fscore=0.423179338722714\n",
      "[(0, 808), (1, 815), (2, 1)]\n",
      "Epoch8: val loss=1.6209608316421509, balanced accuracy=0.3288815526210484, precision=0.41513273233394843, recall=0.42426108374384236, fscore=0.40848616296947066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mweights)\n\u001b[0;32m     35\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(X_train\u001b[38;5;241m.\u001b[39mto(device),y_train\u001b[38;5;241m.\u001b[39mto(device)),batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m---> 36\u001b[0m rnn0 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn0\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRNN_v0.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 32\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[1;34m(model, filename, loss_fn, optimizer, loader, X_val, y_val_nn, y_val, n_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred_batch,y_batch)\n\u001b[0;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\torch-env\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\torch-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HPs\n",
    "hidden_size = 150\n",
    "batch_size = 2\n",
    "lr = 0.0004\n",
    "\n",
    "# Save memory\n",
    "try:\n",
    "    del rnn0\n",
    "    gc.collect()\n",
    "except:\n",
    "    True\n",
    "\n",
    "# # Build and run\n",
    "# Using Module\n",
    "rnn0 = RNN_base(input_size=vec_size,hidden_size=hidden_size,output_size=y_train.shape[1]).to(device)\n",
    "\n",
    "# # Using Sequential -> TBD; for weight-pulling\n",
    "# class generate_h(nn.Module):\n",
    "#     def __init__(self,hidden_size):\n",
    "#         super(generate_h,self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#     def forward(self,x):\n",
    "#         return torch.zeros(1,x.size(0),self.hidden_size).to(device)\n",
    "# class generate\n",
    "\n",
    "# rnn0 = nn.Sequential(\n",
    "#     generate_h(hidden_size),\n",
    "#     nn.RNN(input_size,hidden_size,batch_first=True),\n",
    "#     nn.Linear(hidden_size,output_size)\n",
    "# )\n",
    "\n",
    "optimizer = optim.Adam(rnn0.parameters(),lr=lr)\n",
    "weights = torch.tensor([1, 2, 2.5]).to(device) #hp\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "loader = DataLoader(TensorDataset(X_train.to(device),y_train.to(device)),batch_size=batch_size)\n",
    "rnn0 = train_and_save(rnn0,'RNN_v0.pt',\n",
    "                      loss_fn,optimizer,\n",
    "                      loader,\n",
    "                      X_val.to(device),y_val_nn.to(device),y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfc2af",
   "metadata": {},
   "source": [
    "#### RNN with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "738f1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/mttk/rnn-classifier/blob/master/model.py\n",
    "\n",
    "# Self attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "    \n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        '''\n",
    "        ('query': hidden from rnn) dec_hidden_states: 1 * batch_size * hidden_size\n",
    "        ('key'/'value': output from rnn) enc_hidden_state: batch_size * seq_len * hidden_size\n",
    "        '''\n",
    "        attn_w = torch.bmm(dec_hidden_state.transpose(0,1),enc_hidden_states.transpose(1,2)) # (B*1*H,B*H*L)\n",
    "        attn_w = torch.nn.functional.softmax(attn_w.squeeze(1),dim=1) # need to standardize with ^. (B*1*L -> B*L)\n",
    "        context = torch.bmm(enc_hidden_states.transpose(1,2),attn_w.unsqueeze(2)).squeeze(2) # (B*H*L,B*L*1)\n",
    "        return torch.cat((context,dec_hidden_state.squeeze(0)),dim=1) # batch_size * (2*hidden_size)\n",
    "        \n",
    "class RNN_Attn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN_Attn,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "        self.attn = Attention()\n",
    "        self.dec = nn.Linear(2*hidden_size,output_size) # define context_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1,x.size(0),self.hidden_size).to(device)\n",
    "        # 1: number of RNN layers. x.size(0): batch_size.\n",
    "        output, hidden = self.rnn(x,h0) # hidden: enc_hidden. output: dec_hidden\n",
    "        c_h = self.attn(hidden,output)\n",
    "        output = self.dec(c_h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "666b12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 938), (1, 474), (2, 212)]\n",
      "Epoch1: val loss=0.5653529167175293, balanced accuracy=0.7284039878577694, precision=0.7708903609352612, recall=0.7697044334975369, fscore=0.7698472628980741\n",
      "[(0, 1012), (1, 428), (2, 184)]\n",
      "Epoch2: val loss=0.5234354734420776, balanced accuracy=0.7367851180876391, precision=0.7920420431776465, recall=0.7937192118226601, fscore=0.7910819673189762\n",
      "[(0, 1022), (1, 417), (2, 185)]\n",
      "Epoch3: val loss=0.5023232102394104, balanced accuracy=0.7470958080201777, precision=0.8014673023109354, recall=0.8029556650246306, fscore=0.800157423128351\n",
      "[(0, 1019), (1, 405), (2, 200)]\n",
      "Epoch4: val loss=0.4853055477142334, balanced accuracy=0.758420539933145, precision=0.8029498108731045, recall=0.8048029556650246, fscore=0.8023871071899555\n",
      "[(0, 1033), (1, 395), (2, 196)]\n",
      "Epoch5: val loss=0.4759170114994049, balanced accuracy=0.7572427961083422, precision=0.8068594362351644, recall=0.8084975369458128, fscore=0.8055238614792247\n",
      "[(0, 1029), (1, 392), (2, 203)]\n",
      "Epoch6: val loss=0.46982333064079285, balanced accuracy=0.7604658024826092, precision=0.8071976879761587, recall=0.8091133004926109, fscore=0.8062640610236163\n",
      "[(0, 1015), (1, 398), (2, 211)]\n",
      "Epoch7: val loss=0.4655871093273163, balanced accuracy=0.7664732559690544, precision=0.8083510859430885, recall=0.8103448275862069, fscore=0.8080879323815608\n",
      "[(0, 1016), (1, 394), (2, 214)]\n",
      "Epoch8: val loss=0.46340373158454895, balanced accuracy=0.7648928258171955, precision=0.8063613266872381, recall=0.8084975369458128, fscore=0.8061127610118402\n",
      "[(0, 1019), (1, 397), (2, 208)]\n",
      "Epoch9: val loss=0.46193283796310425, balanced accuracy=0.7649878133071409, precision=0.8095109403803235, recall=0.8115763546798029, fscore=0.8091171281610702\n",
      "[(0, 1026), (1, 391), (2, 207)]\n",
      "Epoch10: val loss=0.460128515958786, balanced accuracy=0.7596968080161357, precision=0.8051107959951913, recall=0.8072660098522167, fscore=0.8044522128340528\n",
      "[(0, 1030), (1, 387), (2, 207)]\n",
      "Epoch11: val loss=0.45924556255340576, balanced accuracy=0.7588853723307505, precision=0.805136416325592, recall=0.8072660098522167, fscore=0.8042637765272018\n",
      "[(0, 1035), (1, 388), (2, 201)]\n",
      "Epoch12: val loss=0.45920562744140625, balanced accuracy=0.7545695045695046, precision=0.8046078026214875, recall=0.8066502463054187, fscore=0.8034356811058481\n",
      "[(0, 1034), (1, 386), (2, 204)]\n",
      "Epoch13: val loss=0.45884597301483154, balanced accuracy=0.755593651602055, precision=0.8039308068696814, recall=0.8060344827586207, fscore=0.8028471981210324\n",
      "[(0, 1037), (1, 384), (2, 203)]\n",
      "Epoch14: val loss=0.45936286449432373, balanced accuracy=0.754163786726812, precision=0.8046232673165657, recall=0.8066502463054187, fscore=0.8033360935196309\n",
      "[(0, 1035), (1, 383), (2, 206)]\n",
      "Epoch15: val loss=0.45951172709465027, balanced accuracy=0.7577369331571013, precision=0.805846750676391, recall=0.8078817733990148, fscore=0.8046590467513896\n",
      "[(0, 1038), (1, 380), (2, 206)]\n",
      "Epoch16: val loss=0.4597165584564209, balanced accuracy=0.7609993492346434, precision=0.8103803037565311, recall=0.812192118226601, fscore=0.8088909850627065\n",
      "[(0, 1028), (1, 383), (2, 213)]\n",
      "Epoch17: val loss=0.463434100151062, balanced accuracy=0.7628905097392492, precision=0.8089152952614278, recall=0.8109605911330049, fscore=0.8080303471289412\n",
      "[(0, 1029), (1, 387), (2, 208)]\n",
      "Epoch18: val loss=0.463888555765152, balanced accuracy=0.7658715001151976, precision=0.8132648528592418, recall=0.8152709359605911, fscore=0.8123541684219012\n",
      "[(0, 1021), (1, 391), (2, 212)]\n",
      "Epoch19: val loss=0.46695205569267273, balanced accuracy=0.7655506647103286, precision=0.8094099583349922, recall=0.8115763546798029, fscore=0.8089591844131615\n",
      "[(0, 1026), (1, 389), (2, 209)]\n",
      "Epoch20: val loss=0.4678790867328644, balanced accuracy=0.7662640813901319, precision=0.8120183188521195, recall=0.8140394088669951, fscore=0.8112662952636878\n",
      "[(0, 1028), (1, 387), (2, 209)]\n",
      "Epoch21: val loss=0.4756802022457123, balanced accuracy=0.7676515050464631, precision=0.813294565158305, recall=0.8152709359605911, fscore=0.8124147206699681\n",
      "[(0, 1026), (1, 385), (2, 213)]\n",
      "Epoch22: val loss=0.4745122790336609, balanced accuracy=0.7683942263774196, precision=0.8127718675985591, recall=0.8146551724137931, fscore=0.8119046028339312\n",
      "[(0, 1010), (1, 397), (2, 217)]\n",
      "Epoch23: val loss=0.47609299421310425, balanced accuracy=0.7690091187990348, precision=0.8089445784660265, recall=0.8109605911330049, fscore=0.8088557436713724\n",
      "Early stopping triggered at epoch 23. Best model is from epoch 13.\n"
     ]
    }
   ],
   "source": [
    "# HPs\n",
    "hidden_size = 70\n",
    "batch_size = 2 # OOM when batch_size >= 4\n",
    "lr = 0.0002\n",
    "\n",
    "# Build and run\n",
    "try:\n",
    "    del rnn0\n",
    "    gc.collect()\n",
    "except:\n",
    "    True\n",
    "\n",
    "rnn_a = RNN_Attn(input_size=vec_size,hidden_size=hidden_size,output_size=y_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(rnn_a.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = DataLoader(TensorDataset(X_train.to(device),y_train.to(device)),batch_size=batch_size)\n",
    "rnn_a = train_and_save(rnn_a,'RNN_Attn_v0.pt',\n",
    "                     loss_fn,optimizer,\n",
    "                     loader,X_val.to(device),y_val_nn.to(device),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(Reference) Getting model weights: \n",
    "https://stackoverflow.com/questions/44130851/simple-lstm-in-pytorch-with-sequential-module\n",
    "https://discuss.pytorch.org/t/how-to-get-all-weights-of-rnn-in-pytorch/33794/2\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee8ae2",
   "metadata": {},
   "source": [
    "#### RNN_Attn with Static Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb74c19",
   "metadata": {},
   "source": [
    "Process Static Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f093e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'citeEnd', 'sectionName', 'citeStart', 'string', 'label',\n",
       "       'label_confidence', 'citingPaperId', 'citedPaperId', 'isKeyCitation',\n",
       "       'id', 'unique_id', 'excerpt_index', 'label2', 'label2_confidence',\n",
       "       'edited_string', 'tagged_string', 'label_num', 'string_lower',\n",
       "       'tokens_lower'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec11ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature Ideas: \n",
    "Include sectionName in text\n",
    "Num: citeStart, citeEnd, excerpt_index, isKeyCitation\n",
    "OHE: label2\n",
    "'''\n",
    "class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names].to_numpy()\n",
    "\n",
    "num_cols = ['citeStart','citeEnd','excerpt_index','isKeyCitation']\n",
    "cat_cols = ['label2']\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(num_cols)),\n",
    "    ('scaler',StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector',DataFrameSelector(cat_cols)),\n",
    "    ('ohe',OneHotEncoder())\n",
    "])\n",
    "combined_pipeline = FeatureUnion([\n",
    "    ('num',num_pipeline),\n",
    "    ('cat',cat_pipeline)\n",
    "])\n",
    "feat_train,feat_test = combined_pipeline.fit_transform(df_train),combined_pipeline.fit_transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0515a0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<8117x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40585 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b3267",
   "metadata": {},
   "source": [
    "Build and Train Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45288041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Attn_StatFeat(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN_Attn,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "        self.attn = Attention()\n",
    "        self.dec = nn.Linear(2*hidden_size,output_size) # define context_size\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1,x.size(0),self.hidden_size).to(device)\n",
    "        # 1: number of RNN layers. x.size(0): batch_size.\n",
    "        output, hidden = self.rnn(x,h0) # hidden: enc_hidden. output: dec_hidden\n",
    "        c_h = self.attn(hidden,output)\n",
    "        output = self.dec(c_h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPs\n",
    "hidden_size = 70\n",
    "batch_size = 2 # OOM when batch_size >= 4\n",
    "lr = 0.0002\n",
    "\n",
    "# Build and run\n",
    "try:\n",
    "    del rnn0\n",
    "    gc.collect()\n",
    "except:\n",
    "    True\n",
    "\n",
    "rnn_a = RNN_Attn(input_size=vec_size,hidden_size=hidden_size,output_size=y_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(rnn_a.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = DataLoader(TensorDataset(X_train.to(device),y_train.to(device)),batch_size=batch_size)\n",
    "rnn_a = train_and_save(rnn_a,'RNN_Attn_v0.pt',\n",
    "                     loss_fn,optimizer,\n",
    "                     loader,X_val.to(device),y_val_nn.to(device),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b3397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df64dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "842bddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Actions taken to address problem of assigning all samples to one class:\n",
    "- Removed outlier samples with extremely high word counts (>100) (worked for Attn. RNN unaffected)\n",
    "- Resampling (tried for RNN; didn't work)\n",
    "- Weighted loss (tried for RNN; didn't work)\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0423cbcb",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fffde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variables:\n",
    "- lower case tokeniation\n",
    "- bert models\n",
    "- hyperparams\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef0e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "epochs = 4\n",
    "max_len1 = 512\n",
    "seed_val = 28\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "config = BertConfig(\n",
    "    max_length = max_len1,\n",
    "    max_position_embeddings = max_len1,\n",
    ") # Doesn't work?\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True,\n",
    "                                          config = config\n",
    "                                         )\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 3\n",
    ")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference\n",
    "\n",
    "max_len = 0\n",
    "for sent in df['edited_string'].to_numpy():\n",
    "    input_ids = tokenizer.encode(sent,add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_torchDS(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for row in tqdm(df['edited_string'].to_numpy()):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_len1,\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        if len(encoded_dict['input_ids'][0]) > 512:\n",
    "            row_input_ids = encoded_dict['input_ids'][0][:512]\n",
    "            row_attention_mask = encoded_dict['input_ids'][0][:512]\n",
    "        else:\n",
    "            row_input_ids = encoded_dict['input_ids'][0]\n",
    "            row_attention_mask = encoded_dict['input_ids'][0]\n",
    "        input_ids.append(row_input_ids)\n",
    "        attention_masks.append(row_attention_mask)\n",
    "    input_ids = torch.from_numpy(np.array(input_ids))\n",
    "    attention_masks = torch.from_numpy(np.array(attention_masks))\n",
    "    labels = torch.tensor(df['label_num'].to_numpy())\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = train_test_split(np.arange(len(df)),test_size=0.2,random_state=28)\n",
    "input_ids_core, attention_masks_core, labels_core = df_to_torchDS(df_train)\n",
    "input_ids_test, attention_masks_test, labels_test = df_to_torchDS(df_test)\n",
    "val_inputs, val_attention, val_labels = input_ids_core[val_idx], attention_masks_core[val_idx], labels_core[val_idx] \n",
    "val_labels = val_labels.numpy()\n",
    "train_data = TensorDataset(input_ids_core[train_idx],attention_masks_core[train_idx],labels_core[train_idx])\n",
    "test_data = TensorDataset(input_ids_test,attention_masks_test,labels_test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler = RandomSampler(train_data),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training Init\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    eps = 1e-8 #epsilon, to prevent division by zero\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d48da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,train_dataloader,val_inputs=None,val_attention=None,val_labels=None):\n",
    "    start_time = time.perf_counter()\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    for epoch in tqdm(range(0,epochs)):\n",
    "        # Training\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_masks = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            result = model(\n",
    "                batch_input_ids,\n",
    "                token_type_ids = None, #KIV\n",
    "                attention_mask = batch_attention_masks,\n",
    "                labels = batch_labels,\n",
    "                return_dict = True\n",
    "            )\n",
    "            loss = result.loss\n",
    "            logits = result.logits #KIV\n",
    "            total_train_loss += loss.item() #KIV \n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(),1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) #KIV\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()      \n",
    "        result = model(\n",
    "                    val_inputs,\n",
    "                    token_type_ids = None, #KIV\n",
    "                    attention_mask = val_attention,\n",
    "                    labels = val_labels,\n",
    "                    return_dict = True\n",
    "                )\n",
    "        val_loss = result.loss.item()\n",
    "        val_pred = result.logits.detach.cpu().numpy()\n",
    "        val_pred = np.argmax(val_pred,axis=1).flatten()\n",
    "        accuracy = accuracy_score(val_labels,val_pred)\n",
    "        precision,recall,f1,_ = precision_recall_fscore_support(val_labels,val_pred)\n",
    "        print(f'Epoch:{epoch}, val_loss:{val_loss}, accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}')\n",
    "    new_path = r'C:\\YZC\\NUS\\Semester 2\\CS4248 Natural Language Processing\\Project\\scicite\\bert'\n",
    "    if not os.path.exists(new_path):\n",
    "        model.save_pretrained(new_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model,optimizer,train_dataloader,val_inputs=val_inputs,val_attention=val_attention,val_labels=val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02734d1a",
   "metadata": {},
   "source": [
    "### Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88991897",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pytorch lightning, lightning fabric\n",
    "'''\n",
    "no_print=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # Draft 1\n",
    "#         total_eval_loss = 0\n",
    "#         for batch in val_dataloader:\n",
    "#             batch_input_ids = batch[0].to(device)\n",
    "#             batch_attention_masks = batch[1].to(device)\n",
    "#             batch_labels = batch[2].to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 result = model(\n",
    "#                     batch_input_ids,\n",
    "#                     token_type_ids = None, #KIV\n",
    "#                     attention_mask = batch_attention_masks,\n",
    "#                     labels = batch_labels,\n",
    "#                     return_dict = True\n",
    "#                 )\n",
    "#             loss = result.loss\n",
    "#             logits = result.logits #KIV\n",
    "#             total_val_loss += loss.item()\n",
    "#             logits = logits.detach().cpu().numpy() #KIV\n",
    "#             labels = batch_labels.to(device).numpy()\n",
    "        \n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
