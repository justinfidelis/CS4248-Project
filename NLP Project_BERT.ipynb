{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4e4946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import transformers\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc107df4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>citeEnd</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>citeStart</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>isKeyCitation</th>\n",
       "      <th>id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>excerpt_index</th>\n",
       "      <th>label2</th>\n",
       "      <th>label2_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explicit</td>\n",
       "      <td>175.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>168.0</td>\n",
       "      <td>However, how frataxin interacts with the Fe-S ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649</td>\n",
       "      <td>894be9b4ea46a5c422e81ef3c241072d4c73fdc0</td>\n",
       "      <td>True</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>explicit</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Novel Quantitative Trait Loci for Seminal Root...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>In the study by Hickey et al. (2012), spikes w...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b</td>\n",
       "      <td>b6642e19efb8db5623b3cc4eef1c5822a6151107</td>\n",
       "      <td>True</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explicit</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>225.0</td>\n",
       "      <td>The drug also reduces catecholamine secretion,...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc</td>\n",
       "      <td>4e6a17fb8d7a3cada601d942e22eb5da6d01adbd</td>\n",
       "      <td>False</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>explicit</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>46.0</td>\n",
       "      <td>By clustering with lowly aggressive close kin ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b</td>\n",
       "      <td>2cc6ff899bf17666ad35893524a4d61624555ed7</td>\n",
       "      <td>False</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explicit</td>\n",
       "      <td>239.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>234.0</td>\n",
       "      <td>Ophthalmic symptoms are rare manifestations of...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175</td>\n",
       "      <td>a5bb0ff1a026944d2a47a155462959af2b8505a8</td>\n",
       "      <td>False</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source  citeEnd                                        sectionName  \\\n",
       "0  explicit    175.0                                       Introduction   \n",
       "1  explicit     36.0  Novel Quantitative Trait Loci for Seminal Root...   \n",
       "2  explicit    228.0                                       Introduction   \n",
       "3  explicit    110.0                                         Discussion   \n",
       "4  explicit    239.0                                         Discussion   \n",
       "\n",
       "   citeStart                                             string       label  \\\n",
       "0      168.0  However, how frataxin interacts with the Fe-S ...  background   \n",
       "1       16.0  In the study by Hickey et al. (2012), spikes w...  background   \n",
       "2      225.0  The drug also reduces catecholamine secretion,...  background   \n",
       "3       46.0  By clustering with lowly aggressive close kin ...  background   \n",
       "4      234.0  Ophthalmic symptoms are rare manifestations of...  background   \n",
       "\n",
       "   label_confidence                             citingPaperId  \\\n",
       "0               1.0  1872080baa7d30ec8fb87be9a65358cd3a7fb649   \n",
       "1               1.0  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b   \n",
       "2               1.0  9cdf605beb1aa1078f235c4332b3024daa8b31dc   \n",
       "3               1.0  d9f3207db0c79a3b154f3875c9760cc6b056904b   \n",
       "4               1.0  88b86556857f4374842d2af2e359576806239175   \n",
       "\n",
       "                               citedPaperId  isKeyCitation  \\\n",
       "0  894be9b4ea46a5c422e81ef3c241072d4c73fdc0           True   \n",
       "1  b6642e19efb8db5623b3cc4eef1c5822a6151107           True   \n",
       "2  4e6a17fb8d7a3cada601d942e22eb5da6d01adbd          False   \n",
       "3  2cc6ff899bf17666ad35893524a4d61624555ed7          False   \n",
       "4  a5bb0ff1a026944d2a47a155462959af2b8505a8          False   \n",
       "\n",
       "                                                  id  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...   \n",
       "\n",
       "                                           unique_id  excerpt_index label2  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...             11    NaN   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...              2    NaN   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...              0    NaN   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...              3    NaN   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...              2    NaN   \n",
       "\n",
       "   label2_confidence  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('/home/jupyter/CS4248-Project/data/scicite/train.jsonl',lines=True)\n",
    "df_test = pd.read_json('/home/jupyter/CS4248-Project/data/scicite/test.jsonl',lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6978616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Questions:\n",
    "excerpt_index?\n",
    "citeStart, citeEnd?\n",
    "label2 (supportiveness)?\n",
    "isKeyCitation?\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e3370",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83433209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def null_test():\n",
    "    null_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any(): null_cols.append(col)\n",
    "    print(null_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de41c53",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f712d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_label(value):\n",
    "    if value == 'background':\n",
    "        label = 0\n",
    "    elif value == 'method':\n",
    "        label = 1\n",
    "    elif value == 'result':\n",
    "        label = 2\n",
    "    return label\n",
    "\n",
    "def NA_impute(df):\n",
    "    df['label2'] = df['label2'].fillna('cant_determine')\n",
    "    try:\n",
    "        print(len(df).df.columns)\n",
    "        df = df.drop(columns='label2_confidence',axis=1)\n",
    "        df_type = 'train'\n",
    "    except:\n",
    "        df_type = 'test'\n",
    "    df['label_confidence'] = df['label_confidence'].fillna(df['label_confidence'].mean())\n",
    "    df['citeStart'] = df['citeStart'].fillna(df['citeStart'].mean().astype(np.int64))\n",
    "    df['citeEnd'] = df['citeEnd'].fillna(df['citeEnd'].mean().astype(np.int64))\n",
    "    df['source'] = df['source'].fillna('unknown')\n",
    "    df['sectionName'] = df['sectionName'].fillna('unknown')\n",
    "    return df, df_type\n",
    "\n",
    "def process_data(df):\n",
    "    df, df_type = NA_impute(df)\n",
    "    for col in ['citeStart','citeEnd']:\n",
    "        df[col] = df[col].astype('int64')\n",
    "    feature_cols = ['source', 'citeEnd', 'sectionName', 'citeStart', 'label_confidence', 'citingPaperId', 'citedPaperId', 'isKeyCitation', 'excerpt_index', 'label2', 'label2_confidence']\n",
    "    if df_type == 'test':\n",
    "        feature_cols.remove('label2_confidence')\n",
    "    df['edited_string'] = ''\n",
    "    for col in feature_cols:\n",
    "        df['edited_string'] += col + ': ' + df[col].astype(str) + '[SEP]'\n",
    "    df['edited_string'] += df['string']\n",
    "    df['label_num'] = df['label'].apply(lambda x: convert_label(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a28734d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = process_data(df)\n",
    "df_test = process_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0423cbcb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fffde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVariables:\\n- lower case tokeniation\\n- bert models\\n- hyperparams\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Variables:\n",
    "- lower case tokeniation\n",
    "- bert models\n",
    "- hyperparams\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef0e119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "epochs = 4\n",
    "max_len1 = 512\n",
    "seed_val = 28\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "config = BertConfig(\n",
    "    max_length = max_len1,\n",
    "    max_position_embeddings = max_len1,\n",
    ") # Doesn't work?\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True,\n",
    "                                          config = config\n",
    "                                         )\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 3\n",
    ")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f71b5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For reference\n",
    "\n",
    "# max_len = 0\n",
    "# for sent in df['edited_string'].to_numpy():\n",
    "#     input_ids = tokenizer.encode(sent,add_special_tokens=True)\n",
    "#     max_len = max(max_len, len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fdaaff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_to_torchDS(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for row in tqdm(df['edited_string'].to_numpy()):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            row,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_len1,\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        if len(encoded_dict['input_ids'][0]) > 512:\n",
    "            row_input_ids = encoded_dict['input_ids'][0][:512]\n",
    "            row_attention_mask = encoded_dict['input_ids'][0][:512]\n",
    "        else:\n",
    "            row_input_ids = encoded_dict['input_ids'][0]\n",
    "            row_attention_mask = encoded_dict['input_ids'][0]\n",
    "        input_ids.append(row_input_ids)\n",
    "        attention_masks.append(row_attention_mask)\n",
    "    input_ids = torch.from_numpy(np.array(input_ids))\n",
    "    attention_masks = torch.from_numpy(np.array(attention_masks))\n",
    "    labels = torch.tensor(df['label_num'].to_numpy())\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9263ff26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e2da401c1a4704b1b39cc080218ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5c75ecec9a4fdca8f4b66e6105ed6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_idx, val_idx = train_test_split(np.arange(len(df)),test_size=0.2,random_state=28)\n",
    "input_ids_core, attention_masks_core, labels_core = df_to_torchDS(df_train)\n",
    "input_ids_test, attention_masks_test, labels_test = df_to_torchDS(df_test)\n",
    "val_inputs, val_attention, val_labels = input_ids_core[val_idx], attention_masks_core[val_idx], labels_core[val_idx] \n",
    "val_labels = val_labels.numpy()\n",
    "train_data = TensorDataset(input_ids_core[train_idx],attention_masks_core[train_idx],labels_core[train_idx])\n",
    "test_data = TensorDataset(input_ids_test,attention_masks_test,labels_test)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler = RandomSampler(train_data),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a7904a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-training Init\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    eps = 1e-8 #epsilon, to prevent division by zero\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d48da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,train_dataloader,val_inputs=None,val_attention=None,val_labels=None):\n",
    "    start_time = time.perf_counter()\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    #     optimizer,\n",
    "    #     num_warmup_steps=0,\n",
    "    #     num_training_steps=total_steps\n",
    "    # )\n",
    "    for epoch in tqdm(range(0,epochs)):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_masks = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            result = model(\n",
    "                batch_input_ids,\n",
    "                token_type_ids = None, #KIV\n",
    "                attention_mask = batch_attention_masks,\n",
    "                labels = batch_labels,\n",
    "                return_dict = True\n",
    "            )\n",
    "            loss = result.loss\n",
    "            logits = result.logits #KIV\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(),1.0)\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            if step % 100 == 0:\n",
    "                print(f'Step {step}, time elapsed:{time.perf_counter()-start_time}')\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()      \n",
    "        result = model(\n",
    "                    val_inputs.to(device),\n",
    "                    token_type_ids = None, #KIV\n",
    "                    attention_mask = val_attention.to(device),\n",
    "                    labels = val_labels,\n",
    "                    return_dict = True\n",
    "                )\n",
    "        val_loss = result.loss.item()\n",
    "        val_pred = result.logits.detach.cpu().numpy()\n",
    "        val_pred = np.argmax(val_pred,axis=1).flatten()\n",
    "        accuracy = accuracy_score(val_labels,val_pred)\n",
    "        precision,recall,f1,_ = precision_recall_fscore_support(val_labels,val_pred)\n",
    "        print(f'Epoch:{epoch}, val_loss:{val_loss}, accuracy:{accuracy}, precision:{precision}, recall:{recall}, f1:{f1}')\n",
    "    new_path = r'/home/jupyter/CS4248-Project/trained_models'\n",
    "    if not os.path.exists(new_path):\n",
    "        model.save_pretrained(new_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008c20a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c88bc5fb764ccfa5a66c414f5defd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, time elapsed:1.7209734150001168\n",
      "Step 100, time elapsed:125.8626514790003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_dataloader, val_inputs, val_attention, val_labels)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;66;03m#KIV\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(),\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model,optimizer,train_dataloader,val_inputs=val_inputs,val_attention=val_attention,val_labels=val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02734d1a",
   "metadata": {},
   "source": [
    "### Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f888b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # Draft 1\n",
    "#         total_eval_loss = 0\n",
    "#         for batch in val_dataloader:\n",
    "#             batch_input_ids = batch[0].to(device)\n",
    "#             batch_attention_masks = batch[1].to(device)\n",
    "#             batch_labels = batch[2].to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 result = model(\n",
    "#                     batch_input_ids,\n",
    "#                     token_type_ids = None, #KIV\n",
    "#                     attention_mask = batch_attention_masks,\n",
    "#                     labels = batch_labels,\n",
    "#                     return_dict = True\n",
    "#                 )\n",
    "#             loss = result.loss\n",
    "#             logits = result.logits #KIV\n",
    "#             total_val_loss += loss.item()\n",
    "#             logits = logits.detach().cpu().numpy() #KIV\n",
    "#             labels = batch_labels.to(device).numpy()\n",
    "        \n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88991897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
