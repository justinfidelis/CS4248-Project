{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LR \n",
      "Vectorizer: count\n",
      "Vector PCA: False\n",
      "Accuracy:  0.7743148844707146\n",
      "F1 Score:  0.7490496883284102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import utils\n",
    "from feature_extractor import FeatureExtractor\n",
    "from log_regression import LogReg\n",
    "\n",
    "model_type = \"LR\"\n",
    "feature_list = {\"word_vector\"}  # {\"word_embedding\", \"word_vector\", \"pos_tag\"}\n",
    "vect = \"count\"\n",
    "vect_pca = False\n",
    "\n",
    "train_x, train_y, test_x, test_y = utils.load_dataset_split()\n",
    "\n",
    "feature_ext = FeatureExtractor(feature_list=feature_list, word_vectorizer=vect, vector_pca=vect_pca, vector_filter=True)\n",
    "\n",
    "train_feat = feature_ext.extract_features(train_x, train=True)\n",
    "test_feat = feature_ext.extract_features(test_x, train=False)\n",
    "\n",
    "model = LogReg()\n",
    "\n",
    "model.train(train_feat, train_y)\n",
    "test_pred = model.predict(test_feat)\n",
    "\n",
    "print(f\"Model: {model_type}\", f\", Features: {feature_list}\" if model_type in {\"MLP\", \"LogReg\"} else \"\")\n",
    "if \"word_vector\" in feature_list:\n",
    "    print(f\"Vectorizer: {vect}\") \n",
    "    print(f\"Vector PCA: {vect_pca}\")\n",
    "print(\"Accuracy: \", accuracy_score(test_y, test_pred))\n",
    "print(\"F1 Score: \", f1_score(test_y, test_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "['background']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Our findings agree with recent work [69,70] where continual organic enrichment from farming processes resulted in increased macrofaunal abundances despite expectations of negative impacts from this contamination.\"\n",
    "input_feat = feature_ext.extract_features([input_sentence]).values\n",
    "print(input_feat[0])\n",
    "print(model.predict(input_feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findings: 1.4090013765407698\n",
      "agree: 0.5147994869489517\n",
      "resulted: 0.5007935899514159\n",
      "despite: 0.4248115525849851\n",
      "recent: 0.41758512182448454\n",
      "work: 0.3453511232274969\n",
      "processes: 0.1503482350107756\n",
      "increased: 0.03920864965984831\n",
      "abundances: 0.020158143374003113\n",
      "organic: 0.004595936270514809\n",
      "enrichment: -0.04803863140994574\n",
      "negative: -0.055026143580853566\n",
      "impacts: -0.05993656065020865\n",
      "farming: -0.08091825069898732\n",
      "expectations: -0.09757888725230988\n",
      "contamination: -0.1591118584135347\n"
     ]
    }
   ],
   "source": [
    "vocab = feature_ext.vectorizer.vocabulary_\n",
    "\n",
    "word_list = [w for w, _ in sorted(vocab.items(), key=lambda x: x[1])]\n",
    "weights = model.model.coef_[2]\n",
    "\n",
    "scores = [(word, weight) for word, idx, weight in zip(word_list, input_feat[0], weights) if idx > 0]\n",
    "\n",
    "scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for w, i in scores:\n",
    "    print(f\"{w}: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Most Important Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background\n",
      "animals: 1.041577023525401\n",
      "term: 0.7800491045017149\n",
      "dysfunction: 0.7767181388714247\n",
      "still: 0.6772008798694836\n",
      "list: 0.6715095311036127\n",
      "lengths: 0.6672990139549911\n",
      "much: 0.6527520836759497\n",
      "typically: 0.6375042832646156\n",
      "investigation: 0.6142547485657774\n",
      "hand: 0.6086005707499181\n",
      "up: 0.6083068046220632\n",
      "expectation: 0.6072804065433052\n",
      "epitopes: 0.599898560051005\n",
      "need: 0.5964572909724665\n",
      "onset: 0.5873570569982723\n",
      "measurements: 0.5822848009582191\n",
      "nine: 0.5819514463366982\n",
      "appear: 0.5810911132941853\n",
      "possibly: 0.5808327292767645\n",
      "inferred: 0.578413380986326\n",
      "\n",
      "method\n",
      "used: 1.7131360357006968\n",
      "method: 1.6881860201332763\n",
      "using: 1.4443434828159694\n",
      "methods: 1.142924393650429\n",
      "we: 1.1399945979375496\n",
      "procedure: 1.1362047318744883\n",
      "technique: 1.1258847077679708\n",
      "described: 1.1137084855999073\n",
      "evaluated: 1.0210772958918055\n",
      "applied: 1.0037976763757348\n",
      "tnf: 0.9673417277832363\n",
      "performed: 0.9618754556167153\n",
      "utilized: 0.9422340125398933\n",
      "measured: 0.8976678779311809\n",
      "selected: 0.8903081032076429\n",
      "were: 0.8760811025253804\n",
      "software: 0.8407079114879467\n",
      "index: 0.8238841712125134\n",
      "calculated: 0.8115306465522307\n",
      "according: 0.8082441887627786\n",
      "\n",
      "result\n",
      "results: 2.4474350510480787\n",
      "consistent: 1.8878985450506853\n",
      "similar: 1.7814745025760488\n",
      "agreement: 1.5169233598847711\n",
      "findings: 1.4509542405577591\n",
      "result: 1.3393570148164626\n",
      "contrast: 1.256035140490618\n",
      "finding: 1.1757545532299807\n",
      "compared: 1.166411518016457\n",
      "previous: 1.1612679458707797\n",
      "line: 1.0890708437017187\n",
      "accordance: 1.0799207446093406\n",
      "supports: 0.995396224011951\n",
      "reported: 0.9923894441410247\n",
      "our: 0.9101585122116904\n",
      "studies: 0.8884595854530255\n",
      "reports: 0.8823493919611275\n",
      "other: 0.8769576270048829\n",
      "comparable: 0.8552240676105058\n",
      "earlier: 0.7495977638139312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = feature_ext.vectorizer.vocabulary_\n",
    "\n",
    "word_list = [w for w, _ in sorted(vocab.items(), key=lambda x: x[1])]\n",
    "\n",
    "for class_name, weights in zip(model.model.classes_, model.model.coef_):\n",
    "    sorted_words = [(word, weight) for word, weight in sorted(zip(word_list, weights), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    print(class_name)\n",
    "    for word, weight in sorted_words[:20]:\n",
    "        print(f\"{word}: {weight}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background\n",
      "animals: 1.0415770235253972\n",
      "term: 0.7800491045017127\n",
      "dysfunction: 0.7767181388714246\n",
      "still: 0.6772008798694839\n",
      "list: 0.6715095311036119\n",
      "lengths: 0.6672990139549906\n",
      "much: 0.6527520836759496\n",
      "typically: 0.6375042832646161\n",
      "investigation: 0.6142547485657788\n",
      "hand: 0.6086005707499184\n",
      "up: 0.6083068046220618\n",
      "expectation: 0.6072804065433044\n",
      "epitopes: 0.5998985600510064\n",
      "need: 0.5964572909724656\n",
      "onset: 0.5873570569982727\n",
      "measurements: 0.5822848009582181\n",
      "nine: 0.5819514463366988\n",
      "appear: 0.5810911132941854\n",
      "possibly: 0.580832729276764\n",
      "inferred: 0.5784133809863267\n",
      "\n",
      "method\n",
      "used: 1.713136035700697\n",
      "method: 1.688186020133271\n",
      "using: 1.444343482815968\n",
      "methods: 1.1429243936504314\n",
      "we: 1.139994597937548\n",
      "procedure: 1.1362047318744897\n",
      "technique: 1.1258847077679721\n",
      "described: 1.1137084855999029\n",
      "evaluated: 1.0210772958918057\n",
      "applied: 1.003797676375734\n",
      "tnf: 0.9673417277832363\n",
      "performed: 0.961875455616713\n",
      "utilized: 0.9422340125398933\n",
      "measured: 0.8976678779311803\n",
      "selected: 0.8903081032076421\n",
      "were: 0.8760811025253797\n",
      "software: 0.840707911487948\n",
      "index: 0.823884171212512\n",
      "calculated: 0.8115306465522308\n",
      "according: 0.8082441887627781\n",
      "\n",
      "result\n",
      "results: 2.4474350510480885\n",
      "consistent: 1.8878985450506853\n",
      "similar: 1.7814745025760503\n",
      "agreement: 1.5169233598847731\n",
      "findings: 1.450954240557759\n",
      "result: 1.3393570148164602\n",
      "contrast: 1.2560351404906178\n",
      "finding: 1.1757545532299838\n",
      "compared: 1.1664115180164585\n",
      "previous: 1.1612679458707784\n",
      "line: 1.089070843701719\n",
      "accordance: 1.0799207446093408\n",
      "supports: 0.9953962240119557\n",
      "reported: 0.9923894441410224\n",
      "our: 0.9101585122116898\n",
      "studies: 0.888459585453025\n",
      "reports: 0.8823493919611295\n",
      "other: 0.876957627004882\n",
      "comparable: 0.8552240676105045\n",
      "earlier: 0.7495977638139331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_weights = np.mean(model.model.coef_, axis=0)\n",
    "\n",
    "norm_weights = model.model.coef_ - mean_weights\n",
    "\n",
    "for class_name, weights in zip(model.model.classes_,norm_weights):\n",
    "    sorted_words = [(word, weight) for word, weight in sorted(zip(word_list, weights), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    print(class_name)\n",
    "    for word, weight in sorted_words[:20]:\n",
    "        print(f\"{word}: {weight}\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
