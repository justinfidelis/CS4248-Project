{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb978f41",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4e4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support,balanced_accuracy_score\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from nltk.tokenize import MWETokenizer,word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "path = r\"data/scicite\"\n",
    "os.chdir(path)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb153f",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc107df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>citeEnd</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>citeStart</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>citingPaperId</th>\n",
       "      <th>citedPaperId</th>\n",
       "      <th>isKeyCitation</th>\n",
       "      <th>id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>excerpt_index</th>\n",
       "      <th>label2</th>\n",
       "      <th>label2_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explicit</td>\n",
       "      <td>175.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>168.0</td>\n",
       "      <td>However, how frataxin interacts with the Fe-S ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649</td>\n",
       "      <td>894be9b4ea46a5c422e81ef3c241072d4c73fdc0</td>\n",
       "      <td>True</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>explicit</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Novel Quantitative Trait Loci for Seminal Root...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>In the study by Hickey et al. (2012), spikes w...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b</td>\n",
       "      <td>b6642e19efb8db5623b3cc4eef1c5822a6151107</td>\n",
       "      <td>True</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explicit</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>225.0</td>\n",
       "      <td>The drug also reduces catecholamine secretion,...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc</td>\n",
       "      <td>4e6a17fb8d7a3cada601d942e22eb5da6d01adbd</td>\n",
       "      <td>False</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>explicit</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>46.0</td>\n",
       "      <td>By clustering with lowly aggressive close kin ...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b</td>\n",
       "      <td>2cc6ff899bf17666ad35893524a4d61624555ed7</td>\n",
       "      <td>False</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>explicit</td>\n",
       "      <td>239.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>234.0</td>\n",
       "      <td>Ophthalmic symptoms are rare manifestations of...</td>\n",
       "      <td>background</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175</td>\n",
       "      <td>a5bb0ff1a026944d2a47a155462959af2b8505a8</td>\n",
       "      <td>False</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source  citeEnd                                        sectionName  \\\n",
       "0  explicit    175.0                                       Introduction   \n",
       "1  explicit     36.0  Novel Quantitative Trait Loci for Seminal Root...   \n",
       "2  explicit    228.0                                       Introduction   \n",
       "3  explicit    110.0                                         Discussion   \n",
       "4  explicit    239.0                                         Discussion   \n",
       "\n",
       "   citeStart                                             string       label  \\\n",
       "0      168.0  However, how frataxin interacts with the Fe-S ...  background   \n",
       "1       16.0  In the study by Hickey et al. (2012), spikes w...  background   \n",
       "2      225.0  The drug also reduces catecholamine secretion,...  background   \n",
       "3       46.0  By clustering with lowly aggressive close kin ...  background   \n",
       "4      234.0  Ophthalmic symptoms are rare manifestations of...  background   \n",
       "\n",
       "   label_confidence                             citingPaperId  \\\n",
       "0               1.0  1872080baa7d30ec8fb87be9a65358cd3a7fb649   \n",
       "1               1.0  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b   \n",
       "2               1.0  9cdf605beb1aa1078f235c4332b3024daa8b31dc   \n",
       "3               1.0  d9f3207db0c79a3b154f3875c9760cc6b056904b   \n",
       "4               1.0  88b86556857f4374842d2af2e359576806239175   \n",
       "\n",
       "                               citedPaperId  isKeyCitation  \\\n",
       "0  894be9b4ea46a5c422e81ef3c241072d4c73fdc0           True   \n",
       "1  b6642e19efb8db5623b3cc4eef1c5822a6151107           True   \n",
       "2  4e6a17fb8d7a3cada601d942e22eb5da6d01adbd          False   \n",
       "3  2cc6ff899bf17666ad35893524a4d61624555ed7          False   \n",
       "4  a5bb0ff1a026944d2a47a155462959af2b8505a8          False   \n",
       "\n",
       "                                                  id  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...   \n",
       "\n",
       "                                           unique_id  excerpt_index label2  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...             11    NaN   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...              2    NaN   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...              0    NaN   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...              3    NaN   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...              2    NaN   \n",
       "\n",
       "   label2_confidence  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('train.jsonl',lines=True)\n",
    "df_test = pd.read_json('test.jsonl',lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6978616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Questions:\n",
    "excerpt_index?\n",
    "citeStart, citeEnd?\n",
    "label2 (supportiveness)?\n",
    "isKeyCitation?\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e3370",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83433209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_test():\n",
    "    null_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any(): null_cols.append(col)\n",
    "    print(null_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de41c53",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f712d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = MWETokenizer([('<','bos','>'),('<','eos','>')],separator='')\n",
    "# tk.add_mwe([('<','bos','>'),('<','eos','>')])\n",
    "\n",
    "def convert_label(value):\n",
    "    if value == 'background':\n",
    "        label = 0\n",
    "    elif value == 'method':\n",
    "        label = 1\n",
    "    elif value == 'result':\n",
    "        label = 2\n",
    "    return label\n",
    "\n",
    "def NA_impute(df):\n",
    "    df['label2'] = df['label2'].fillna('cant_determine')\n",
    "    try:\n",
    "        print(len(df).df.columns)\n",
    "        df = df.drop(columns='label2_confidence',axis=1)\n",
    "        df_type = 'train'\n",
    "    except:\n",
    "        df_type = 'test'\n",
    "    df['label_confidence'] = df['label_confidence'].fillna(df['label_confidence'].mean())\n",
    "    df['citeStart'] = df['citeStart'].fillna(df['citeStart'].mean().astype(np.int64))\n",
    "    df['citeEnd'] = df['citeEnd'].fillna(df['citeEnd'].mean().astype(np.int64))\n",
    "    df['source'] = df['source'].fillna('unknown')\n",
    "    df['sectionName'] = df['sectionName'].fillna('unknown')\n",
    "    return df, df_type\n",
    "\n",
    "def add_sectionName(df):\n",
    "    df['string_lower_sn'] = df.apply(lambda x:x.sectionName.lower()+' '+x.string_lower,axis=1)\n",
    "    df['tokens_lower_sn'] = df['string_lower_sn'].apply(lambda x: tk.tokenize(word_tokenize(x)))\n",
    "    return df\n",
    "\n",
    "def process_df(df):\n",
    "    df, df_type = NA_impute(df)\n",
    "    for col in ['citeStart','citeEnd']:\n",
    "        df[col] = df[col].astype('int64')\n",
    "    feature_cols = ['source', 'citeEnd', 'sectionName', 'citeStart', 'label_confidence', 'citingPaperId', 'citedPaperId', 'isKeyCitation', 'excerpt_index', 'label2', 'label2_confidence']\n",
    "    if df_type == 'test':\n",
    "        feature_cols.remove('label2_confidence')\n",
    "    df['edited_string'] = ''\n",
    "    for col in feature_cols:\n",
    "        df['edited_string'] += col + ': ' + df[col].astype(str) + '[SEP]'\n",
    "    df['edited_string'] += df['string']\n",
    "    df['tagged_string'] = '<BOS>' + df['string'] + '<EOS>'\n",
    "    df['label_num'] = df['label'].apply(lambda x: convert_label(x))\n",
    "    df['string_lower'] = df['tagged_string'].apply(lambda x: x.lower())\n",
    "    df['tokens_lower'] = df['string_lower'].apply(lambda x: tk.tokenize(word_tokenize(x)))  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a28734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = process_df(df)\n",
    "df_test = process_df(df_test)\n",
    "# Remove outliers, i.e. lengthy sentences\n",
    "df_train = df_train.loc[df_train['tokens_lower'].str.len() <= 100]\n",
    "df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38280d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hypotheses:\n",
    "Word embeddings\n",
    "Combine word embeddings with features\n",
    "Attention mechanism\n",
    "Include the sentences immediately before and after\n",
    "Activation functions\n",
    "\n",
    "Open questions\n",
    "- Where can we get more contextual information? How to incorporate?\n",
    "- variables:\n",
    "    - lower case? stopwords? lemmatization? any words to include?\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762ebf8",
   "metadata": {},
   "source": [
    "#### Preparing Embedding Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4319d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input array (batch_first = True): N (batch size) * L (sequence length) * H_in (input size)\n",
    "'''\n",
    "def generate_X(df,token_col,model,seq_len,vec_size):\n",
    "    '''\n",
    "    token_col: col for tokens in dataframe\n",
    "    model: word vectorization model\n",
    "    '''\n",
    "    X = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        i_arr = [model.wv[token] for token in df.at[i,token_col]]\n",
    "        if len(i_arr) < seq_len:\n",
    "            while len(i_arr) < seq_len:\n",
    "                i_arr.append(np.zeros(vec_size))\n",
    "        elif len(i_arr) > seq_len:\n",
    "            i_arr = i_arr[:seq_len]\n",
    "        X.append(i_arr)\n",
    "    return np.array(X)\n",
    "\n",
    "class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names].to_numpy()\n",
    "\n",
    "def generate_static_features(df):\n",
    "    num_cols = ['citeStart','citeEnd','excerpt_index','isKeyCitation']\n",
    "    cat_cols = ['label2']\n",
    "    num_pipeline = Pipeline([\n",
    "        ('selector',DataFrameSelector(num_cols)),\n",
    "        ('scaler',StandardScaler())\n",
    "    ])\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('selector',DataFrameSelector(cat_cols)),\n",
    "        ('ohe',OneHotEncoder())\n",
    "    ])\n",
    "    combined_pipeline = FeatureUnion([\n",
    "        ('num',num_pipeline),\n",
    "        ('cat',cat_pipeline)\n",
    "    ])\n",
    "    sf = combined_pipeline.fit_transform(df)\n",
    "    return sf\n",
    "\n",
    "def convert_y(y): # for training and validation labels. For y_val, keep one multi-target and one single-target version\n",
    "    y_nn = np.zeros([y.shape[0],3])\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            y_nn[i,0] = 1\n",
    "        elif y[i] == 1:\n",
    "            y_nn[i,1] = 1\n",
    "        elif y[i] == 2: \n",
    "            y_nn[i,2] = 1\n",
    "    return y_nn\n",
    "\n",
    "def to_tensor(arr,dtype='Float'):\n",
    "    if dtype == \"Float\":\n",
    "        arr = torch.from_numpy(arr).type(torch.FloatTensor)\n",
    "    elif dtype == \"Long\":\n",
    "        arr = torch.from_numpy(arr).type(torch.LongTensor)\n",
    "    return arr\n",
    "\n",
    "def check_dist_y(y,n=None):\n",
    "    '''\n",
    "    y: torch array\n",
    "    n: last index of sample\n",
    "    '''\n",
    "    if n == None:\n",
    "        print(torch.sum(y_train,dim=0))\n",
    "    else:\n",
    "        print(torch.sum(y_train[:n],dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e9188c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 200 # hp\n",
    "seq_length = 100 # max word length based on tokenization, including padding tokens. Outliers removed (~ 1.5% of dataset)\n",
    "corpus = df_train['tokens_lower'].tolist()\n",
    "ft = FastText(corpus,vector_size=vec_size,epochs=10)\n",
    "sf_core = generate_static_features(df_train)\n",
    "sf_test = generate_static_features(df_test)\n",
    "y_core = df_train['label_num'].to_numpy()\n",
    "y_test = df_test['label_num'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1de9a",
   "metadata": {},
   "source": [
    "#####  For Subsequent Runs (with Tensors Saved), Ignore this Cell:\n",
    "*i.e. Only run once*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd156264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8117/8117 [00:04<00:00, 2026.66it/s]\n",
      "100%|██████████| 1861/1861 [00:02<00:00, 680.51it/s] \n"
     ]
    }
   ],
   "source": [
    "X_core = generate_X(df_train,'tokens_lower',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_test = generate_X(df_test,'tokens_lower',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_train, X_val, sf_train, sf_val, y_train, y_val = train_test_split(X_core,sf_core,y_core,test_size=0.2,random_state= 1)\n",
    "y_train, y_val_nn = convert_y(y_train),convert_y(y_val)\n",
    "X_train, X_val, X_test, sf_train, sf_val, sf_test = to_tensor(X_train), to_tensor(X_val), to_tensor(X_test), to_tensor(sf_train.toarray()), to_tensor(sf_val.toarray()), to_tensor(sf_test.toarray()),\n",
    "y_train, y_val_nn = to_tensor(y_train), to_tensor(y_val_nn)\n",
    "torch.save([X_train,X_val,X_test,sf_train,sf_val,sf_test,y_train,y_val_nn],'data_arrays_v0.pt') # embeddings only\n",
    "np.savez('y_arr_v0.npz',y_val=y_val,y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8846f0",
   "metadata": {},
   "source": [
    "##### For Subsequent Runs (Tensors Saved), Resume from this Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc06333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train,X_val,X_test,sf_train,sf_val,sf_test,y_train,y_val_nn = torch.load('data_arrays_v0.pt')\n",
    "data = np.load('y_arr_v0.npz')\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Utility/Essential parameters\n",
    "vec_size = 200 # hp\n",
    "seq_length = 100 # max word length based on tokenization, including padding tokens\n",
    "models_generated = ['rnn_0','rnn_a','rnn_a_sf','rnn_a_sf1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b9262e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dist(arr):\n",
    "    unique, counts = np.unique(arr,return_counts=True)\n",
    "    return list(zip(unique,counts))\n",
    "    \n",
    "def resample_arr(X,y,rng): # Not used\n",
    "    y_0 = y[:,0] == 1 # class 0\n",
    "    y_non0 =  y[:,0] == 0 # class 1 or 2\n",
    "    idx_0 = y_0.nonzero().numpy() # list of lists (numpy)\n",
    "    idx_non0 = y_non0.nonzero() # list of lists (torch)\n",
    "    idx_sample_0 = rng.choice(idx_0,len(idx_0)//3) # as size of class 0 ~ 4x that of other classes\n",
    "    idx_sample_0 = torch.from_numpy(idx_sample_0).type(torch.LongTensor)\n",
    "    idx_sample = torch.cat((idx_sample_0.squeeze(1),idx_non0.squeeze(1))) \n",
    "    del y_0,y_non0,idx_0,idx_non0,idx_sample_0\n",
    "    return X[idx_sample], y[idx_sample]\n",
    "\n",
    "def train_and_save(model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs):\n",
    "    '''\n",
    "    sf: [bool] whether static features are included or not\n",
    "    '''\n",
    "    X_val, sf_val = kwargs.get('X_val', None),kwargs.get('sf_val', None)\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "        # Data resampling to offset class imbalance\n",
    "#         X_sample, y_sample = resample_arr(X_train,y_train,rng)\n",
    "#         check_dist(y_sample.numpy()[:,0]);check_dist(y_sample.numpy()[:,1]); check_dist(y_sample.numpy()[:,2]) # Troubleshooting\n",
    "#         loader = DataLoader(TensorDataset(X_sample.to(device),y_sample.to(device)),batch_size=batch_size)\n",
    "\n",
    "        if sf == False:\n",
    "            for X_batch, y_batch in loader:\n",
    "                y_pred_batch = model(X_batch)\n",
    "                loss = loss_fn(y_pred_batch,y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        else:\n",
    "            for X_batch, sf_batch, y_batch in loader:\n",
    "                y_pred_batch = model(X_batch,sf_batch)\n",
    "                loss = loss_fn(y_pred_batch,y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if sf == False:\n",
    "                y_pred_nn = model(X_val)\n",
    "            else:\n",
    "                y_pred_nn = model(X_val,sf_val)\n",
    "            y_pred = torch.argmax(y_pred_nn,1).detach().cpu()\n",
    "            y_dist = check_dist(y_pred) # Troubleshooting \n",
    "            print(y_dist) # Troubleshooting\n",
    "            bal_accuracy = balanced_accuracy_score(y_val,y_pred)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                precision, recall, fscore, support = precision_recall_fscore_support(y_val,y_pred,average='weighted')\n",
    "            val_loss = loss_fn(y_pred_nn,y_val_nn)\n",
    "            tqdm.write(f\"Epoch{epoch+1}: val loss={val_loss}, balanced accuracy={bal_accuracy}, precision={precision}, recall={recall}, fscore={fscore}\")\n",
    "            \n",
    "            # Early Stopping\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_epoch = epoch + 1\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                patience = 10\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print(f'Early stopping triggered at epoch {epoch+1}. Best model is from epoch {best_epoch}.')\n",
    "                    break\n",
    "        \n",
    "        # For memory saving\n",
    "        try:\n",
    "            del X_sample,y_sample\n",
    "            gc.collect()\n",
    "        except:\n",
    "            True\n",
    "    torch.save(best_model_weights,filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737358e0",
   "metadata": {},
   "source": [
    "#### Naive RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc540fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "class RNN_base(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN_base,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "        self.ff = nn.Linear(hidden_size,output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1,x.size(0),self.hidden_size).to(device)\n",
    "        # 1: number of RNN layers. x.size(0): batch_size.\n",
    "        _, hidden = self.rnn(x,h0)\n",
    "        output = self.ff(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aa8dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5898660095487448 0.2782997073771754 0.13183428307407977\n"
     ]
    }
   ],
   "source": [
    "# For reference\n",
    "class0 = check_dist(y_train[:,0])[1][1]\n",
    "class1 = check_dist(y_train[:,1])[1][1]\n",
    "class2 = check_dist(y_train[:,2])[1][1]\n",
    "print(class0/len(y_train),class1/len(y_train),class2/len(y_train))\n",
    "print(class0 + class1 + class2 == len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPs\n",
    "hidden_size = 150\n",
    "batch_size = 2\n",
    "lr = 0.0004\n",
    "\n",
    "# Save memory\n",
    "for var in models_generated:\n",
    "    if var in globals(): del var\n",
    "gc.collect()\n",
    "\n",
    "# # Build and train\n",
    "# Building Approach 1: Using Module\n",
    "rnn0 = RNN_base(input_size=vec_size,hidden_size=hidden_size,output_size=y_train.shape[1]).to(device)\n",
    "\n",
    "# # Building Apprach 2: Using Sequential -> WIP: to enable weight analysis\n",
    "# class generate_h(nn.Module):\n",
    "#     def __init__(self,hidden_size):\n",
    "#         super(generate_h,self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#     def forward(self,x):\n",
    "#         return torch.zeros(1,x.size(0),self.hidden_size).to(device)\n",
    "# class generate\n",
    "\n",
    "# rnn0 = nn.Sequential(\n",
    "#     generate_h(hidden_size),\n",
    "#     nn.RNN(input_size,hidden_size,batch_first=True),\n",
    "#     nn.Linear(hidden_size,output_size)\n",
    "# )\n",
    "\n",
    "optimizer = optim.Adam(rnn0.parameters(),lr=lr)\n",
    "# Weighted loss required for RNN to not assign all labels to one class\n",
    "weights = torch.tensor([1, 2, 2.5]).to(device) #hp\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "loader = DataLoader(TensorDataset(X_train.to(device),y_train.to(device)),batch_size=batch_size)\n",
    "'''\n",
    "train_and_save args: model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs\n",
    "'''\n",
    "rnn0 = train_and_save(rnn0,'RNN_v0.pt',\n",
    "                      loss_fn,optimizer,\n",
    "                      loader,y_val_nn.to(device),y_val,X_val=X_val.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfc2af",
   "metadata": {},
   "source": [
    "#### RNN with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/mttk/rnn-classifier/blob/master/model.py\n",
    "\n",
    "# Self attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "    \n",
    "    def forward(self,dec_hidden_state,enc_hidden_states):\n",
    "        '''\n",
    "        ('query': hidden from rnn) dec_hidden_states: 1 * batch_size * hidden_size\n",
    "        ('key'/'value': output from rnn) enc_hidden_state: batch_size * seq_len * hidden_size\n",
    "        '''\n",
    "        attn_w = torch.bmm(dec_hidden_state.transpose(0,1),enc_hidden_states.transpose(1,2)) # (B*1*H,B*H*L)\n",
    "        attn_w = torch.nn.functional.softmax(attn_w.squeeze(1),dim=1) # need to standardize with ^. (B*1*L -> B*L)\n",
    "        context = torch.bmm(enc_hidden_states.transpose(1,2),attn_w.unsqueeze(2)).squeeze(2) # (B*H*L,B*L*1)\n",
    "        return torch.cat((context,dec_hidden_state.squeeze(0)),dim=1) # batch_size * (2*hidden_size)\n",
    "        \n",
    "class RNN_Attn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(RNN_Attn,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "        self.attn = Attention()\n",
    "        self.dec = nn.Linear(2*hidden_size,output_size) # define context_size\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1,x.size(0),self.hidden_size).to(device) # 1: number of RNN layers. x.size(0): batch_size.\n",
    "        output, hidden = self.rnn(x,h0) # hidden: enc_hidden. output: dec_hidden\n",
    "        c_h = self.attn(hidden,output)\n",
    "        output = self.dec(c_h)\n",
    "#         output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPs\n",
    "hidden_size = 70\n",
    "batch_size = 2 # OOM when batch_size >= 4\n",
    "lr = 0.0002\n",
    "\n",
    "# Save memory\n",
    "for var in models_generated:\n",
    "    if var in globals(): del var\n",
    "gc.collect()\n",
    "\n",
    "# Build and train\n",
    "rnn_a = RNN_Attn(input_size=vec_size,hidden_size=hidden_size,output_size=y_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(rnn_a.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = DataLoader(TensorDataset(X_train.to(device),y_train.to(device)),batch_size=batch_size)\n",
    "'''\n",
    "train_and_save args: model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs\n",
    "'''\n",
    "rnn_a = train_and_save(rnn_a,'RNN_Attn_v0.pt',\n",
    "                     loss_fn,optimizer,\n",
    "                     loader,X_val.to(device),y_val_nn.to(device),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(Reference) Getting model weights: \n",
    "https://stackoverflow.com/questions/44130851/simple-lstm-in-pytorch-with-sequential-module\n",
    "https://discuss.pytorch.org/t/how-to-get-all-weights-of-rnn-in-pytorch/33794/2\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee8ae2",
   "metadata": {},
   "source": [
    "#### RNN_Attn with Static Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b3267",
   "metadata": {},
   "source": [
    "Build and Train Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45288041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Attn_StatFeat(nn.Module):\n",
    "    def __init__(self,input_size,feat_size,hidden_size,hidden_size2,output_size):\n",
    "        super(RNN_Attn_StatFeat,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "        self.attn = Attention()\n",
    "        self.dec = nn.Linear(2*hidden_size,hidden_size2) # define context_size\n",
    "        self.ff1 = nn.Linear(hidden_size2+feat_size,output_size) # combined layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x,sf): # x: embeddings. sf: static features\n",
    "        h0 = torch.zeros(1,x.size(0),self.hidden_size).to(device) # 1: number of RNN layers. x.size(0): batch_size.\n",
    "        output, hidden = self.rnn(x,h0) # hidden: enc_hidden. output: dec_hidden\n",
    "        context_h = self.attn(hidden,output)\n",
    "        context_h2 = self.dec(context_h)\n",
    "        output = self.ff1(torch.cat((context_h2,sf),dim=1))\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5962da8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e651f3d78e404c9a9037cccfa7219610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1617), (1, 7)]\n",
      "Epoch1: val loss=0.946342945098877, balanced accuracy=0.3333333333333333, precision=0.3451261382295865, recall=0.5862068965517241, fscore=0.4344639669266185\n",
      "[(0, 1440), (1, 183), (2, 1)]\n",
      "Epoch2: val loss=0.9002696871757507, balanced accuracy=0.42468654128317995, precision=0.577262342189561, recall=0.6483990147783252, fscore=0.5685871006804291\n",
      "[(0, 1329), (1, 294), (2, 1)]\n",
      "Epoch3: val loss=0.8863833546638489, balanced accuracy=0.4574329731892757, precision=0.5705987501250988, recall=0.6631773399014779, fscore=0.5981036318441268\n",
      "[(0, 1151), (1, 297), (2, 176)]\n",
      "Epoch4: val loss=0.8215010166168213, balanced accuracy=0.6578262618178584, precision=0.7356138425584221, recall=0.7376847290640394, fscore=0.7245454724733096\n",
      "[(0, 1023), (1, 428), (2, 173)]\n",
      "Epoch5: val loss=0.7423332333564758, balanced accuracy=0.7528385091410302, precision=0.8094902690683033, recall=0.8084975369458128, fscore=0.8060462515042427\n",
      "[(0, 948), (1, 494), (2, 182)]\n",
      "Epoch6: val loss=0.7397065162658691, balanced accuracy=0.7704056370022757, precision=0.8136599883265898, recall=0.8078817733990148, fscore=0.8085763115114083\n",
      "[(0, 989), (1, 436), (2, 199)]\n",
      "Epoch7: val loss=0.725704550743103, balanced accuracy=0.7861321296195246, precision=0.8235996493530181, recall=0.8238916256157636, fscore=0.8229276592690542\n",
      "[(0, 955), (1, 486), (2, 183)]\n",
      "Epoch8: val loss=0.7290248870849609, balanced accuracy=0.7859517544391496, precision=0.8270457493384933, recall=0.8220443349753694, fscore=0.8225387999027199\n",
      "[(0, 1022), (1, 411), (2, 191)]\n",
      "Epoch9: val loss=0.7191099524497986, balanced accuracy=0.7866692131398013, precision=0.8299034009631301, recall=0.8300492610837439, fscore=0.8280373794426952\n",
      "[(0, 993), (1, 449), (2, 182)]\n",
      "Epoch10: val loss=0.7180708646774292, balanced accuracy=0.7935371118144228, precision=0.8358558229272102, recall=0.833128078817734, fscore=0.8325854504617862\n",
      "[(0, 980), (1, 453), (2, 191)]\n",
      "Epoch11: val loss=0.7165752649307251, balanced accuracy=0.8063538546731824, precision=0.8398826859223619, recall=0.8374384236453202, fscore=0.8374340466051299\n",
      "[(0, 939), (1, 511), (2, 174)]\n",
      "Epoch12: val loss=0.7214718461036682, balanced accuracy=0.8003130545147353, precision=0.8428769220931013, recall=0.833743842364532, fscore=0.8348405064303703\n",
      "[(0, 932), (1, 520), (2, 172)]\n",
      "Epoch13: val loss=0.7221619486808777, balanced accuracy=0.8018672115310771, precision=0.84429773996383, recall=0.833128078817734, fscore=0.8347083899160116\n",
      "[(0, 984), (1, 453), (2, 187)]\n",
      "Epoch14: val loss=0.7081751823425293, balanced accuracy=0.8063801278086992, precision=0.8427057030265639, recall=0.8399014778325123, fscore=0.8397913363819272\n",
      "[(0, 995), (1, 437), (2, 192)]\n",
      "Epoch15: val loss=0.7075334787368774, balanced accuracy=0.8096491121701206, precision=0.843349312475143, recall=0.8417487684729064, fscore=0.8412553788930869\n",
      "[(0, 973), (1, 457), (2, 194)]\n",
      "Epoch16: val loss=0.7091169357299805, balanced accuracy=0.8150937142533782, precision=0.8456371554007678, recall=0.8429802955665024, fscore=0.8432566696484711\n",
      "[(0, 942), (1, 496), (2, 186)]\n",
      "Epoch17: val loss=0.7136688828468323, balanced accuracy=0.8100149150569319, precision=0.8429030088279261, recall=0.8355911330049262, fscore=0.8371166933588462\n",
      "[(0, 963), (1, 470), (2, 191)]\n",
      "Epoch18: val loss=0.709786593914032, balanced accuracy=0.8179337391522266, precision=0.8472220952281504, recall=0.8429802955665024, fscore=0.843768156109281\n",
      "[(0, 967), (1, 463), (2, 194)]\n",
      "Epoch19: val loss=0.7077248692512512, balanced accuracy=0.8203584464088666, precision=0.8481631379767319, recall=0.8448275862068966, fscore=0.8454055178401321\n",
      "[(0, 962), (1, 467), (2, 195)]\n",
      "Epoch20: val loss=0.7091841697692871, balanced accuracy=0.8178518882300395, precision=0.8447728863312559, recall=0.8411330049261084, fscore=0.8418910581249581\n",
      "[(0, 967), (1, 467), (2, 190)]\n",
      "Epoch21: val loss=0.7064264416694641, balanced accuracy=0.8186471558320297, precision=0.8493594974187919, recall=0.8454433497536946, fscore=0.8460344402072825\n",
      "[(0, 952), (1, 463), (2, 209)]\n",
      "Epoch22: val loss=0.7099359631538391, balanced accuracy=0.8203614779245031, precision=0.8414000434541332, recall=0.8392857142857143, fscore=0.8399180979990409\n",
      "[(0, 983), (1, 443), (2, 198)]\n",
      "Epoch23: val loss=0.7022691369056702, balanced accuracy=0.8207677010198019, precision=0.8492637631304794, recall=0.8479064039408867, fscore=0.8477248916756677\n",
      "[(0, 980), (1, 429), (2, 215)]\n",
      "Epoch24: val loss=0.7038100361824036, balanced accuracy=0.8237289865441126, precision=0.8466562221144647, recall=0.8472906403940886, fscore=0.8466689386646004\n",
      "[(0, 984), (1, 446), (2, 194)]\n",
      "Epoch25: val loss=0.703636109828949, balanced accuracy=0.8139452750797288, precision=0.8453538495866724, recall=0.8435960591133005, fscore=0.8434073502428391\n",
      "[(0, 981), (1, 447), (2, 196)]\n",
      "Epoch26: val loss=0.7006700038909912, balanced accuracy=0.8223349946039021, precision=0.8504187158176045, recall=0.8485221674876847, fscore=0.8485153648855035\n",
      "[(0, 975), (1, 449), (2, 200)]\n",
      "Epoch27: val loss=0.7032047510147095, balanced accuracy=0.8219717179801213, precision=0.8482583644467812, recall=0.8466748768472906, fscore=0.8467412118426172\n",
      "[(0, 983), (1, 443), (2, 198)]\n",
      "Epoch28: val loss=0.6984923481941223, balanced accuracy=0.8262158398713021, precision=0.8537155461638228, recall=0.8522167487684729, fscore=0.8520975882590819\n",
      "[(0, 1026), (1, 380), (2, 218)]\n",
      "Epoch29: val loss=0.6968145370483398, balanced accuracy=0.819888056232594, precision=0.8485338182885594, recall=0.8497536945812808, fscore=0.8471742309103825\n",
      "[(0, 991), (1, 429), (2, 204)]\n",
      "Epoch30: val loss=0.7025163769721985, balanced accuracy=0.8157449848626319, precision=0.8425192870460163, recall=0.8423645320197044, fscore=0.8417249365538538\n",
      "[(0, 1015), (1, 408), (2, 201)]\n",
      "Epoch31: val loss=0.7092645764350891, balanced accuracy=0.8025422290128171, precision=0.8362922857174175, recall=0.8368226600985221, fscore=0.8351441897894328\n",
      "[(0, 1069), (1, 360), (2, 195)]\n",
      "Epoch32: val loss=0.7080181241035461, balanced accuracy=0.7895688578461687, precision=0.836110970675884, recall=0.8349753694581281, fscore=0.8309136062438578\n",
      "[(0, 1052), (1, 374), (2, 198)]\n",
      "Epoch33: val loss=0.7048251032829285, balanced accuracy=0.8015251555167522, precision=0.8438969253148176, recall=0.8435960591133005, fscore=0.8403617769749266\n",
      "[(0, 1011), (1, 419), (2, 194)]\n",
      "Epoch34: val loss=0.7015004754066467, balanced accuracy=0.8137133641335321, precision=0.848661326385287, recall=0.8479064039408867, fscore=0.846749496505196\n",
      "[(0, 1017), (1, 418), (2, 189)]\n",
      "Epoch35: val loss=0.7012688517570496, balanced accuracy=0.8089230641751649, precision=0.8479014283635407, recall=0.8466748768472906, fscore=0.8453502492889778\n",
      "[(0, 963), (1, 469), (2, 192)]\n",
      "Epoch36: val loss=0.7200007438659668, balanced accuracy=0.8028554856285949, precision=0.8302895491640919, recall=0.8257389162561576, fscore=0.8267650077992033\n",
      "[(0, 1075), (1, 352), (2, 197)]\n",
      "Epoch37: val loss=0.7093313932418823, balanced accuracy=0.7944869867138774, precision=0.8409530802513943, recall=0.8392857142857143, fscore=0.8349173885690213\n",
      "[(0, 1433), (1, 22), (2, 169)]\n",
      "Epoch38: val loss=0.8504262566566467, balanced accuracy=0.5889997413106657, precision=0.7402905811732844, recall=0.6988916256157636, fscore=0.6066427555346645\n",
      "[(0, 1352), (1, 105), (2, 167)]\n",
      "Epoch39: val loss=0.8302481174468994, balanced accuracy=0.6229638319974454, precision=0.7416089775152815, recall=0.7222906403940886, fscore=0.6679992325501661\n",
      "Early stopping triggered at epoch 39. Best model is from epoch 29.\n"
     ]
    }
   ],
   "source": [
    "# HPs\n",
    "hidden_size = 70\n",
    "hidden_size2 = 50\n",
    "batch_size = 2 # OOM when batch_size >= 4\n",
    "feat_size = sf_train.shape[1]\n",
    "lr = 0.0002\n",
    "\n",
    "# Save memory\n",
    "for var in models_generated:\n",
    "    if var in globals(): del var\n",
    "gc.collect()\n",
    "\n",
    "# Build and train\n",
    "'''\n",
    "RNN_Attn_StatFeat args: input_size,feat_size,hidden_size,hidden_size2,output_size\n",
    "'''\n",
    "rnn_a_sf = RNN_Attn_StatFeat(input_size=vec_size,feat_size=feat_size,\n",
    "                             hidden_size=hidden_size,hidden_size2=hidden_size2,\n",
    "                             output_size=y_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(rnn_a_sf.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = DataLoader(TensorDataset(X_train.to(device),sf_train.to(device),y_train.to(device)),batch_size=batch_size)\n",
    "'''\n",
    "train_and_save args: model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs\n",
    "'''\n",
    "rnn_a_sf = train_and_save(rnn_a_sf,'RNN_Attn_sf.pt',\n",
    "                     loss_fn,optimizer,\n",
    "                     loader,y_val_nn.to(device),y_val,sf=True,X_val=X_val.to(device),sf_val=sf_val.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe56797",
   "metadata": {},
   "source": [
    "#### RNN_Attn_StatFeat with Section Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68c084",
   "metadata": {},
   "source": [
    "##### For Subsequent Runs (Tensors Saved), Ignore this Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96ee0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8117/8117 [00:16<00:00, 485.72it/s]\n",
      "100%|██████████| 1861/1861 [00:01<00:00, 961.34it/s] \n"
     ]
    }
   ],
   "source": [
    "# Generic inclusion\n",
    "df_train, df_test = add_sectionName(df_train), add_sectionName(df_test)\n",
    "df_train = df_train.loc[df_train['tokens_lower'].str.len() <= 100]\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "corpus_sn = df_train['tokens_lower_sn'].tolist()\n",
    "ft_sn = FastText(corpus_sn,vector_size=vec_size,epochs=10)\n",
    "# unchanged: sf_core, y_core. Note:\n",
    "    # sf_core is not a tensor (sf_train and sf_val definitely are, but not sf_train_sn and sf_val_sn)\n",
    "    # sf_test and y_test no change, as no changes to features and not implicated by the repeat of train-val splits\n",
    "X_core_sn = generate_X(df_train,'tokens_lower_sn',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_test_sn = generate_X(df_test,'tokens_lower_sn',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_train_sn, X_val_sn, sf_train_sn, sf_val_sn, y_train_sn, y_val_sn = train_test_split(X_core_sn,sf_core,y_core,test_size=0.2,random_state= 1)\n",
    "y_train_sn, y_val_sn_nn = convert_y(y_train_sn),convert_y(y_val_sn)\n",
    "if torch.is_tensor(sf_test):\n",
    "    X_train_sn, X_val_sn, X_test_sn, sf_train_sn, sf_val_sn = to_tensor(X_train_sn), to_tensor(X_val_sn), to_tensor(X_test_sn), to_tensor(sf_train_sn.toarray()), to_tensor(sf_val_sn.toarray())\n",
    "else:\n",
    "    X_train_sn, X_val_sn, X_test_sn, sf_train_sn, sf_val_sn, sf_test = to_tensor(X_train_sn), to_tensor(X_val_sn), to_tensor(X_test_sn), to_tensor(sf_train_sn.toarray()), to_tensor(sf_val_sn.toarray()), to_tensor(sf_test.toarray())\n",
    "y_train_sn, y_val_sn_nn = to_tensor(y_train_sn), to_tensor(y_val_sn_nn)\n",
    "torch.save([X_train_sn,X_val_sn,X_test_sn,sf_train_sn,sf_val_sn,sf_test,y_train_sn,y_val_sn_nn],'data_arrays_sn_v0.pt') # embeddings only\n",
    "np.savez('y_arr_sn_v0.npz',y_val=y_val_sn,y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70462d4",
   "metadata": {},
   "source": [
    "##### For Subsequent Runs (Tensors Saved), Resume from this Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc76c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sn,X_val_sn,X_test_sn,sf_train_sn,sf_val_sn,sf_test,y_train_sn,y_val_sn_nn = torch.load('data_arrays_sn_v0.pt')\n",
    "data_sn = np.load('y_arr_sn_v0.npz')\n",
    "y_val_sn = data_sn['y_val']\n",
    "y_test = data_sn['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e01f138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:32<21:04, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1149), (1, 475)]\n",
      "Epoch1: val loss=0.8001357316970825, balanced accuracy=0.5732848695033569, precision=0.6441212748717025, recall=0.750615763546798, fscore=0.6928264623775358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [01:02<19:49, 31.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1147), (1, 477)]\n",
      "Epoch2: val loss=0.7956319451332092, balanced accuracy=0.5743908674580943, precision=0.6450996035597374, recall=0.7518472906403941, fscore=0.6939665238703127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [01:59<26:24, 42.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1183), (1, 441)]\n",
      "Epoch3: val loss=0.7899011969566345, balanced accuracy=0.5727290916366546, precision=0.6512045646661031, recall=0.7567733990147784, fscore=0.6982778809658402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [02:50<27:31, 45.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1153), (1, 470), (2, 1)]\n",
      "Epoch4: val loss=0.7921645641326904, balanced accuracy=0.5757914276821839, precision=0.6479180440319966, recall=0.7543103448275862, fscore=0.6964723977264257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [03:25<24:30, 42.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1146), (1, 477), (2, 1)]\n",
      "Epoch5: val loss=0.7927519083023071, balanced accuracy=0.5736905873460495, precision=0.6444596406726004, recall=0.750615763546798, fscore=0.6930775696097446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [04:04<23:16, 41.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1199), (1, 424), (2, 1)]\n",
      "Epoch6: val loss=0.7916719913482666, balanced accuracy=0.5691332088390912, precision=0.6523632572660041, recall=0.7561576354679803, fscore=0.6977316593595723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [04:43<22:12, 40.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1197), (1, 426), (2, 1)]\n",
      "Epoch7: val loss=0.7899195551872253, balanced accuracy=0.5750689164554711, precision=0.6578384655584241, recall=0.7623152709359606, fscore=0.703624550956153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [05:20<20:56, 39.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1014), (1, 440), (2, 170)]\n",
      "Epoch8: val loss=0.7084484100341797, balanced accuracy=0.7910997732426304, precision=0.8467024116272245, recall=0.8448275862068966, fscore=0.8426619667663816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [05:58<20:11, 39.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1012), (1, 427), (2, 185)]\n",
      "Epoch9: val loss=0.700186014175415, balanced accuracy=0.8093974963722862, precision=0.8543016746349507, recall=0.853448275862069, fscore=0.8519193146592459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [06:35<19:08, 38.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 991), (1, 455), (2, 178)]\n",
      "Epoch10: val loss=0.6967411637306213, balanced accuracy=0.8109319485369905, precision=0.8563989369590223, recall=0.854064039408867, fscore=0.8530201056332176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [07:09<17:53, 37.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1002), (1, 436), (2, 186)]\n",
      "Epoch11: val loss=0.6950381398200989, balanced accuracy=0.8174926536271073, precision=0.8584395071589022, recall=0.8571428571428571, fscore=0.8560683070382221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [07:37<20:05, 41.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(X_train_sn\u001b[38;5;241m.\u001b[39mto(device),sf_train_sn\u001b[38;5;241m.\u001b[39mto(device),y_train_sn\u001b[38;5;241m.\u001b[39mto(device)),batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mtrain_and_save args: model,filename,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m                   loss_fn,optimizer,\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m                   loader,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m rnn_a_sf1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_a_sf1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRNN_Attn_sf_v1.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val_sn_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val_sn\u001b[49m\u001b[43m,\u001b[49m\u001b[43msf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val_sn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msf_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msf_val_sn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 47\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(model, filename, loss_fn, optimizer, loader, y_val_nn, y_val, n_epochs, sf, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred_batch,y_batch)\n\u001b[1;32m     46\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HPs\n",
    "hidden_size = 70\n",
    "hidden_size2 = 50\n",
    "batch_size = 2 # OOM when batch_size >= 4\n",
    "feat_size = sf_train_sn.shape[1]\n",
    "lr = 0.0002\n",
    "\n",
    "# Save memory\n",
    "for var in models_generated:\n",
    "    if var in globals(): del var\n",
    "gc.collect()\n",
    "\n",
    "# Build and train\n",
    "'''\n",
    "RNN_Attn_StatFeat args: input_size,feat_size,hidden_size,hidden_size2,output_size\n",
    "'''\n",
    "rnn_a_sf1 = RNN_Attn_StatFeat(input_size=vec_size,feat_size=feat_size,\n",
    "                             hidden_size=hidden_size,hidden_size2=hidden_size2,\n",
    "                             output_size=y_train_sn.shape[1]).to(device)\n",
    "optimizer = optim.Adam(rnn_a_sf1.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = DataLoader(TensorDataset(X_train_sn.to(device),sf_train_sn.to(device),y_train_sn.to(device)),batch_size=batch_size)\n",
    "'''\n",
    "train_and_save args: model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs\n",
    "'''\n",
    "rnn_a_sf1 = train_and_save(rnn_a_sf1,'RNN_Attn_sf_v1.pt',\n",
    "                     loss_fn,optimizer,\n",
    "                     loader,y_val_sn_nn.to(device),y_val_sn,sf=True,X_val=X_val_sn.to(device),sf_val=sf_val_sn.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204dfcbe",
   "metadata": {},
   "source": [
    "#### Word Order\n",
    "Based on RNN_Attn_StatFeat with sectionName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07851a3",
   "metadata": {},
   "source": [
    "##### For Subsequent Runs (Tensors Saved), Ignore this Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcf86f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8117/8117 [00:11<00:00, 719.97it/s] \n",
      "100%|██████████| 1861/1861 [00:03<00:00, 533.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generic inclusion\n",
    "def randomize_tokens(tk_list,rng):\n",
    "    '''\n",
    "    Based on tokens with sectionName\n",
    "    Keep positions of: sectionName bos, eos\n",
    "    '''\n",
    "    target_list = tk_list[2:-1]\n",
    "    reordered_list = tk_list[:2] + rng.permutation(target_list).tolist() + [tk_list[-1]]\n",
    "    return reordered_list\n",
    "\n",
    "def generate_reordered_tokens(df):\n",
    "    rng1 = np.random.default_rng()\n",
    "    df['tokens_lower_sn_jumbled'] = df['tokens_lower_sn'].apply(lambda x:randomize_tokens(x,rng1))\n",
    "    return df\n",
    "\n",
    "def randomize_ngrams(tk_list, rng, n):\n",
    "    target_list = tk_list[2:-1]\n",
    "    ngrams = [target_list[i:i + n] for i in range(0, len(target_list), n)]\n",
    "    last_ngram = ngrams[-1]\n",
    "    last_ngram.extend(['[BUF]']*(n-len(last_ngram))) # ensure last ngram same size as others for rng to work\n",
    "    reordered_ngrams = rng.permutation(ngrams).tolist()\n",
    "    flattened_ngrams = [token for ngram in reordered_ngrams for token in ngram]\n",
    "    filtered_list = list(filter(lambda x: x != 'BUF', flattened_ngrams))\n",
    "    reordered_list = tk_list[:2] + filtered_list + [tk_list[-1]]\n",
    "    return reordered_list\n",
    "\n",
    "def generate_ngrams(df, n=5):\n",
    "    rng1 = np.random.default_rng()\n",
    "    df['tokens_lower_sn_ngrams'] = df['tokens_lower_sn'].apply(lambda x:randomize_ngrams(x, rng1, n))\n",
    "    return df\n",
    "\n",
    "# Regenerate col again (in case the cell above hasn't been run)\n",
    "df_train, df_test = add_sectionName(df_train), add_sectionName(df_test)\n",
    "df_train = df_train.loc[df_train['tokens_lower'].str.len() <= 100]\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "# df_train, df_test = generate_reordered_tokens(df_train), generate_reordered_tokens(df_test)\n",
    "print(len(df_train))\n",
    "df_train, df_test = generate_ngrams(df_train, n=len(df_train)//5), generate_ngrams(df_test, n=len(df_test)//5)\n",
    "# unchanged: sf_core, y_core. Note:\n",
    "    # sf_core is not a tensor (sf_train and sf_val definitely are, but not sf_train_sn and sf_val_sn)\n",
    "    # y_test has already been loaded\n",
    "    # sf_test and y_test no change, as no changes to features and not implicated by the repeat of train-val splits\n",
    "X_core_sn_ro = generate_X(df_train,'tokens_lower_sn',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_test_sn_ro = generate_X(df_test,'tokens_lower_sn',\n",
    "                   ft,seq_length,vec_size)\n",
    "X_train_sn_ro, X_val_sn_ro, sf_train_sn_ro, sf_val_sn_ro, y_train_sn_ro, y_val_sn_ro = train_test_split(X_core_sn_ro,sf_core,y_core,test_size=0.2,random_state= 1)\n",
    "y_train_sn_ro, y_val_sn_ro_nn = convert_y(y_train_sn_ro),convert_y(y_val_sn_ro)\n",
    "if torch.is_tensor(sf_test): # Depends on whether sf_test has been loaded as or converted to tensor previously\n",
    "    X_train_sn_ro, X_val_sn_ro, X_test_sn_ro, sf_train_sn_ro, sf_val_sn_ro = to_tensor(X_train_sn_ro), to_tensor(X_val_sn_ro), to_tensor(X_test_sn_ro), to_tensor(sf_train_sn_ro.toarray()), to_tensor(sf_val_sn_ro.toarray())\n",
    "else:\n",
    "    X_train_sn_ro, X_val_sn_ro, X_test_sn_ro, sf_train_sn_ro, sf_val_sn_ro, sf_test = to_tensor(X_train_sn_ro), to_tensor(X_val_sn_ro), to_tensor(X_test_sn_ro), to_tensor(sf_train_sn_ro.toarray()), to_tensor(sf_val_sn_ro.toarray()), to_tensor(sf_test.toarray())\n",
    "y_train_sn_ro, y_val_sn_ro_nn = to_tensor(y_train_sn_ro), to_tensor(y_val_sn_ro_nn)\n",
    "torch.save([X_train_sn_ro,X_val_sn_ro,X_test_sn_ro,sf_train_sn_ro,sf_val_sn_ro,sf_test,y_train_sn_ro,y_val_sn_ro_nn],'data_arrays_sn_ro_v0.pt') # embeddings only\n",
    "np.savez('y_arr_sn_ro_v0.npz',y_val=y_val_sn_ro,y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001150e",
   "metadata": {},
   "source": [
    "##### For Subsequent Runs (Tensors Saved), Resume from this Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d4fb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sn_ro,X_val_sn_ro,X_test_sn_ro,sf_train_sn_ro,sf_val_sn_ro,sf_test,y_train_sn_ro,y_val_sn_ro_nn = torch.load('data_arrays_sn_ro_v0.pt')\n",
    "data_sn = np.load('y_arr_sn_ro_v0.npz')\n",
    "y_val_sn_ro = data_sn['y_val']\n",
    "y_test = data_sn['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d6b47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:36<23:29, 36.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1621), (1, 3)]\n",
      "Epoch1: val loss=0.9522787928581238, balanced accuracy=0.33484504913076346, precision=0.52530898338616, recall=0.5874384236453202, fscore=0.4362349587750636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [01:11<22:43, 35.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1425), (1, 199)]\n",
      "Epoch2: val loss=0.9170416593551636, balanced accuracy=0.43218954248366015, precision=0.5778227647614965, recall=0.6551724137931034, fscore=0.5770383300179159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [01:44<21:14, 34.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1355), (1, 269)]\n",
      "Epoch3: val loss=0.8779624104499817, balanced accuracy=0.46052865590680714, precision=0.5839049989333376, recall=0.6693349753694581, fscore=0.6029886920584837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [02:17<20:14, 33.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1187), (1, 249), (2, 188)]\n",
      "Epoch4: val loss=0.7965524792671204, balanced accuracy=0.673834180136701, precision=0.7567907746528606, recall=0.7536945812807881, fscore=0.7358628590470336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [02:52<19:59, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 951), (1, 495), (2, 178)]\n",
      "Epoch5: val loss=0.7290163636207581, balanced accuracy=0.7817404739673647, precision=0.8273126238039697, recall=0.8226600985221675, fscore=0.8224611618213806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [03:29<20:01, 35.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 980), (1, 449), (2, 195)]\n",
      "Epoch6: val loss=0.7077786326408386, balanced accuracy=0.8081798375916023, precision=0.8453236933889068, recall=0.8448275862068966, fscore=0.8441331919214358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [04:03<19:10, 34.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 929), (1, 502), (2, 193)]\n",
      "Epoch7: val loss=0.7107329964637756, balanced accuracy=0.8152043645741124, precision=0.8453462016024249, recall=0.8392857142857143, fscore=0.840411200937267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [04:36<18:11, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 974), (1, 444), (2, 206)]\n",
      "Epoch8: val loss=0.6967216730117798, balanced accuracy=0.827639136462666, precision=0.8544053850236907, recall=0.854064039408867, fscore=0.8537732354500772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [05:09<17:31, 33.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 979), (1, 429), (2, 216)]\n",
      "Epoch9: val loss=0.6910523176193237, balanced accuracy=0.8405412670118553, precision=0.8628149114757987, recall=0.8633004926108374, fscore=0.86277746475446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [05:43<16:53, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 996), (1, 410), (2, 218)]\n",
      "Epoch10: val loss=0.6881459951400757, balanced accuracy=0.8441179502103872, precision=0.8669095313205506, recall=0.8676108374384236, fscore=0.8665988454698561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [06:19<16:40, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 986), (1, 420), (2, 218)]\n",
      "Epoch11: val loss=0.6857755184173584, balanced accuracy=0.8483585353333253, precision=0.869469708952863, recall=0.8700738916256158, fscore=0.8693755934301448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [06:59<16:52, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 989), (1, 410), (2, 225)]\n",
      "Epoch12: val loss=0.6884109377861023, balanced accuracy=0.8408423975650866, precision=0.861104264232036, recall=0.8620689655172413, fscore=0.8610879341590677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [07:37<16:32, 36.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1004), (1, 407), (2, 213)]\n",
      "Epoch13: val loss=0.6836774349212646, balanced accuracy=0.8451946435139712, precision=0.8715423421223824, recall=0.8719211822660099, fscore=0.8708014492533174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [08:15<16:06, 37.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 984), (1, 425), (2, 215)]\n",
      "Epoch14: val loss=0.6831976175308228, balanced accuracy=0.8520822470402303, precision=0.8733798629863274, recall=0.8737684729064039, fscore=0.8731948017665765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [08:52<15:25, 37.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1028), (1, 393), (2, 203)]\n",
      "Epoch15: val loss=0.6813697218894958, balanced accuracy=0.8372273151684917, precision=0.8719812851545262, recall=0.8713054187192119, fscore=0.8696226365483174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [09:28<14:44, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 992), (1, 455), (2, 177)]\n",
      "Epoch16: val loss=0.6902587413787842, balanced accuracy=0.8202771007392856, precision=0.8616532946936549, recall=0.8577586206896551, fscore=0.857306512496974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [10:08<14:24, 37.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1015), (1, 419), (2, 190)]\n",
      "Epoch17: val loss=0.6842361092567444, balanced accuracy=0.828634484096669, precision=0.8665861909362926, recall=0.8651477832512315, fscore=0.8640210709519502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [10:55<14:51, 40.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1053), (1, 387), (2, 184)]\n",
      "Epoch18: val loss=0.6849590539932251, balanced accuracy=0.8214689916370589, precision=0.8697786233680351, recall=0.8663793103448276, fscore=0.8642303463312061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [11:36<14:16, 40.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1005), (1, 414), (2, 205)]\n",
      "Epoch19: val loss=0.6799502968788147, balanced accuracy=0.8408525026172086, precision=0.8689437429992684, recall=0.8688423645320197, fscore=0.8678470579981464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [12:12<13:03, 39.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1031), (1, 410), (2, 183)]\n",
      "Epoch20: val loss=0.685183048248291, balanced accuracy=0.8211708925994641, precision=0.8647301467720212, recall=0.8620689655172413, fscore=0.8606240103769516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [12:48<12:07, 38.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1067), (1, 383), (2, 174)]\n",
      "Epoch21: val loss=0.6900904774665833, balanced accuracy=0.8058086871112081, precision=0.8639091704574781, recall=0.8589901477832512, fscore=0.856278406946492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [13:27<11:34, 38.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1004), (1, 438), (2, 182)]\n",
      "Epoch22: val loss=0.6891644597053528, balanced accuracy=0.8184480863052291, precision=0.8585789300091985, recall=0.8559113300492611, fscore=0.8551638634986899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [14:11<11:20, 40.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1044), (1, 407), (2, 173)]\n",
      "Epoch23: val loss=0.6907511949539185, balanced accuracy=0.8084915784495617, precision=0.8626643186312725, recall=0.8589901477832512, fscore=0.8568911802666381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [14:47<10:22, 38.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1057), (1, 396), (2, 171)]\n",
      "Epoch24: val loss=0.6887561082839966, balanced accuracy=0.8046698477370745, precision=0.8623716442229741, recall=0.8577586206896551, fscore=0.8553329031913185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [15:19<09:13, 36.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1066), (1, 376), (2, 182)]\n",
      "Epoch25: val loss=0.6914238333702087, balanced accuracy=0.8073825489791876, precision=0.8609184308345948, recall=0.8571428571428571, fscore=0.8542941065519691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [16:02<09:02, 38.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1073), (1, 395), (2, 156)]\n",
      "Epoch26: val loss=0.7031577825546265, balanced accuracy=0.7799326801427641, precision=0.8519535841718596, recall=0.8454433497536946, fscore=0.8418384883363288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [17:31<11:39, 53.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1035), (1, 414), (2, 175)]\n",
      "Epoch27: val loss=0.6921879649162292, balanced accuracy=0.8060537346251632, precision=0.8586225934651722, recall=0.8559113300492611, fscore=0.8538838514193922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [18:11<09:54, 49.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1032), (1, 427), (2, 165)]\n",
      "Epoch28: val loss=0.697123646736145, balanced accuracy=0.8012174566796416, precision=0.8580630672082945, recall=0.853448275862069, fscore=0.8515480163144233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [18:41<08:00, 40.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 982), (1, 457), (2, 185)]\n",
      "Epoch29: val loss=0.6982401609420776, balanced accuracy=0.8136835542297728, precision=0.8512005186494405, recall=0.8485221674876847, fscore=0.8482130875705938\n",
      "Early stopping triggered at epoch 29. Best model is from epoch 19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HPs\n",
    "hidden_size = 70\n",
    "hidden_size2 = 50\n",
    "batch_size = 2 # OOM when batch_size >= 4\n",
    "feat_size = sf_train_sn_ro.shape[1]\n",
    "lr = 0.0002\n",
    "\n",
    "# Save memory\n",
    "for var in models_generated:\n",
    "    if var in globals(): del var\n",
    "gc.collect()\n",
    "\n",
    "# Build and train\n",
    "'''\n",
    "RNN_Attn_StatFeat args: input_size,feat_size,hidden_size,hidden_size2,output_size\n",
    "'''\n",
    "rnn_a_sf2 = RNN_Attn_StatFeat(input_size=vec_size,feat_size=feat_size,\n",
    "                             hidden_size=hidden_size,hidden_size2=hidden_size2,\n",
    "                             output_size=y_train_sn_ro.shape[1]).to(device)\n",
    "optimizer = optim.Adam(rnn_a_sf2.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = DataLoader(TensorDataset(X_train_sn_ro.to(device),sf_train_sn_ro.to(device),y_train_sn_ro.to(device)),batch_size=batch_size)\n",
    "'''\n",
    "train_and_save args: model,filename,\n",
    "                   loss_fn,optimizer,\n",
    "                   loader,\n",
    "                   y_val_nn,y_val,n_epochs=40,sf=False,**kwargs\n",
    "'''\n",
    "rnn_a_sf2 = train_and_save(rnn_a_sf2,'RNN_Attn_sf_v2.pt',\n",
    "                     loss_fn,optimizer,\n",
    "                     loader,y_val_sn_ro_nn.to(device),y_val_sn_ro,sf=True,X_val=X_val_sn_ro.to(device),sf_val=sf_val_sn_ro.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96550f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2afa562",
   "metadata": {},
   "source": [
    "#### Other Hyperparam-like Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd14c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To test:\n",
    "- 1 more hidden layer for RNN_Attn_StatFeat?\n",
    "'''\n",
    "no_print=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "842bddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Results:\n",
    "- RNN_Attn - Epoch13: val loss=0.45884597301483154, balanced accuracy=0.755593651602055, precision=0.8039308068696814, recall=0.8060344827586207, fscore=0.8028471981210324\n",
    "- RNN_Attn_StatFeat - Epoch29: val loss=0.6968145370483398, balanced accuracy=0.819888056232594, precision=0.8485338182885594, recall=0.8497536945812808, fscore=0.8471742309103825\n",
    "- RNN_Attn_StatFeat with sectionName - Epoch26: val loss=0.6687561273574829, balanced accuracy=0.8488238729835368, precision=0.8824253380976532, recall=0.8811576354679803, fscore=0.879580416187504\n",
    "- RNN_Attn_StatFeat with sectionName, randomized ngrams - Epoch40: val loss=0.6920244693756104, balanced accuracy=0.8208106474913198, precision=0.8647667864909244, recall=0.8608374384236454, fscore=0.8600928393653015\n",
    "- RNN_Attn_StatFeat with sectionName, randomized ngrams, no test data randomization - Epoch24: val loss=0.6812471151351929, balanced accuracy=0.8263280059498547, precision=0.8717889618210934, recall=0.8682266009852216, fscore=0.8665472144701037\n",
    "- RNN_Attn_StatFeat with sectionName, randomized ngrams, n/5-sized ngrams - Epoch19: val loss=0.6799502968788147, balanced accuracy=0.8408525026172086, precision=0.8689437429992684, recall=0.8688423645320197, fscore=0.8678470579981464\n",
    "\n",
    "\n",
    "\n",
    "Observations:\n",
    "- Results (esp for RNN_Attn) are worse with softmax than without\n",
    "\n",
    "Actions taken to address problem of assigning all samples to one class:\n",
    "- Removed outlier samples with extremely high word counts (>100) (worked for Attn. RNN unaffected)\n",
    "- Resampling (tried for RNN; didn't work)\n",
    "- Weighted loss (tried for RNN; didn't work)\n",
    "'''\n",
    "no_print = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02734d1a",
   "metadata": {},
   "source": [
    "### Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88991897",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pytorch lightning, lightning fabric\n",
    "'''\n",
    "no_print=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888b7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
