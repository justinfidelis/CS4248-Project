{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justi\\OneDrive\\Documents\\NUS\\Year 4 Sem 2\\CS4248\\Project\\CS4248-Project\\src\\Linearity Analysis.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/OneDrive/Documents/NUS/Year%204%20Sem%202/CS4248/Project/CS4248-Project/src/Linearity%20Analysis.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m vect_pca \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/OneDrive/Documents/NUS/Year%204%20Sem%202/CS4248/Project/CS4248-Project/src/Linearity%20Analysis.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m feature_ext \u001b[39m=\u001b[39m FeatureExtractor(feature_list\u001b[39m=\u001b[39mfeature_list, word_vectorizer\u001b[39m=\u001b[39mvect, vector_pca\u001b[39m=\u001b[39mvect_pca)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/justi/OneDrive/Documents/NUS/Year%204%20Sem%202/CS4248/Project/CS4248-Project/src/Linearity%20Analysis.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_feat \u001b[39m=\u001b[39m feature_ext\u001b[39m.\u001b[39;49mextract_features(train_x, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/OneDrive/Documents/NUS/Year%204%20Sem%202/CS4248/Project/CS4248-Project/src/Linearity%20Analysis.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m test_feat \u001b[39m=\u001b[39m feature_ext\u001b[39m.\u001b[39mextract_features(test_x, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\justi\\OneDrive\\Documents\\NUS\\Year 4 Sem 2\\CS4248\\Project\\CS4248-Project\\src\\feature_extractor.py:180\u001b[0m, in \u001b[0;36mFeatureExtractor.extract_features\u001b[1;34m(self, sentences, train)\u001b[0m\n\u001b[0;32m    178\u001b[0m     feature_ls\u001b[39m.\u001b[39mappend(pos_tag_feats)\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mword_vector\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_list:\n\u001b[1;32m--> 180\u001b[0m     wv_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_word_vector_features(sentences, train\u001b[39m=\u001b[39;49mtrain, pca\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvector_pca)\n\u001b[0;32m    181\u001b[0m     feature_ls\u001b[39m.\u001b[39mappend(wv_feats)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mword_embedding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_list:\n",
      "File \u001b[1;32mc:\\Users\\justi\\OneDrive\\Documents\\NUS\\Year 4 Sem 2\\CS4248\\Project\\CS4248-Project\\src\\feature_extractor.py:127\u001b[0m, in \u001b[0;36mFeatureExtractor.get_word_vector_features\u001b[1;34m(self, sentences, train, pca)\u001b[0m\n\u001b[0;32m    125\u001b[0m word_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorizer\u001b[39m.\u001b[39mfit_transform(sentences)\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m pca:\n\u001b[1;32m--> 127\u001b[0m     word_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_vector_svd\u001b[39m.\u001b[39;49mfit_transform(word_vectors)\n\u001b[0;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     word_vectors \u001b[39m=\u001b[39m word_vectors\u001b[39m.\u001b[39mtodense()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:246\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m>\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m    242\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_components(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components\u001b[39m}\u001b[39;00m\u001b[39m) must be <=\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m n_features(\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         )\n\u001b[1;32m--> 246\u001b[0m     U, Sigma, VT \u001b[39m=\u001b[39m randomized_svd(\n\u001b[0;32m    247\u001b[0m         X,\n\u001b[0;32m    248\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_components,\n\u001b[0;32m    249\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter,\n\u001b[0;32m    250\u001b[0m         n_oversamples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_oversamples,\n\u001b[0;32m    251\u001b[0m         power_iteration_normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    252\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    253\u001b[0m     )\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m VT\n\u001b[0;32m    257\u001b[0m \u001b[39m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:450\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m transpose:\n\u001b[0;32m    447\u001b[0m     \u001b[39m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     M \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mT\n\u001b[1;32m--> 450\u001b[0m Q \u001b[39m=\u001b[39m randomized_range_finder(\n\u001b[0;32m    451\u001b[0m     M,\n\u001b[0;32m    452\u001b[0m     size\u001b[39m=\u001b[39;49mn_random,\n\u001b[0;32m    453\u001b[0m     n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m    454\u001b[0m     power_iteration_normalizer\u001b[39m=\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    455\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    456\u001b[0m )\n\u001b[0;32m    458\u001b[0m \u001b[39m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    459\u001b[0m B \u001b[39m=\u001b[39m safe_sparse_dot(Q\u001b[39m.\u001b[39mT, M)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:279\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLU\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    278\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 279\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39;49mlu(safe_sparse_dot(A\u001b[39m.\u001b[39;49mT, Q), permute_l\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    280\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQR\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    281\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mqr(safe_sparse_dot(A, Q), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meconomic\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\linalg\\_decomp_lu.py:313\u001b[0m, in \u001b[0;36mlu\u001b[1;34m(a, permute_l, overwrite_a, check_finite, p_indices)\u001b[0m\n\u001b[0;32m    311\u001b[0m     p \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(m, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)\n\u001b[0;32m    312\u001b[0m     u \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros([k, k], dtype\u001b[39m=\u001b[39ma1\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 313\u001b[0m     lu_dispatcher(a1, u, p, permute_l)\n\u001b[0;32m    314\u001b[0m     P, L, U \u001b[39m=\u001b[39m (p, a1, u) \u001b[39mif\u001b[39;00m m \u001b[39m>\u001b[39m n \u001b[39melse\u001b[39;00m (p, u, a1)\n\u001b[0;32m    316\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Stacked array\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m     \u001b[39m# Prepare the contiguous data holders\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import load_dataset_split\n",
    "from feature_extractor import FeatureExtractor\n",
    "from log_regression import LogReg\n",
    "from mlp import MLP\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = load_dataset_split()\n",
    "\n",
    "feature_list = {\"word_vector\"}\n",
    "vect = \"count\"\n",
    "vect_pca = True\n",
    "\n",
    "feature_ext = FeatureExtractor(feature_list=feature_list, word_vectorizer=vect, vector_pca=vect_pca)\n",
    "\n",
    "train_feat = feature_ext.extract_features(train_x, train=True)\n",
    "test_feat = feature_ext.extract_features(test_x, train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "276/276 [==============================] - 2s 2ms/step - loss: 0.6354\n",
      "Epoch 2/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.4777\n",
      "Epoch 3/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.4404\n",
      "Epoch 4/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.4004\n",
      "Epoch 5/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.3667\n",
      "Epoch 6/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.3278\n",
      "Epoch 7/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.2916\n",
      "Epoch 8/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.2567\n",
      "Epoch 9/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.2130\n",
      "Epoch 10/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.1799\n",
      "Epoch 11/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.1503\n",
      "Epoch 12/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.1270\n",
      "Epoch 13/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.1117\n",
      "Epoch 14/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0963\n",
      "Epoch 15/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0782\n",
      "Epoch 16/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0702\n",
      "Epoch 17/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0594\n",
      "Epoch 18/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0452\n",
      "Epoch 19/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0438\n",
      "Epoch 20/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0358\n",
      "Epoch 21/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0602\n",
      "Epoch 22/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0446\n",
      "Epoch 23/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0506\n",
      "Epoch 24/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0554\n",
      "Epoch 25/25\n",
      "276/276 [==============================] - 1s 2ms/step - loss: 0.0254\n",
      "69/69 [==============================] - 0s 2ms/step\n",
      "Model: MLP , Features: {'word_vector'}\n",
      "Vectorizer: count\n",
      "Vector PCA: True\n",
      "Accuracy:  0.7776769509981851\n",
      "F1 Score:  0.7341539036563628\n"
     ]
    }
   ],
   "source": [
    "model_type = \"MLP\"\n",
    "model_mlp = MLP(layer_sizes=[100, 60, 30])\n",
    "\n",
    "model_mlp.train(train_feat, train_y, epochs=25)\n",
    "mlp_test_pred = model_mlp.predict(test_feat)\n",
    "\n",
    "print(f\"Model: {model_type}\", f\", Features: {feature_list}\" if model_type in {\"MLP\", \"LogReg\"} else \"\")\n",
    "if \"word_vector\" in feature_list:\n",
    "    print(f\"Vectorizer: {vect}\") \n",
    "    print(f\"Vector PCA: {vect_pca}\")\n",
    "print(\"Accuracy: \", accuracy_score(test_y, mlp_test_pred))\n",
    "print(\"F1 Score: \", f1_score(test_y, mlp_test_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LR \n",
      "Vectorizer: count\n",
      "Vector PCA: True\n",
      "Accuracy:  0.8103448275862069\n",
      "F1 Score:  0.7676904613248025\n"
     ]
    }
   ],
   "source": [
    "model_type = \"LR\"\n",
    "model_lr = LogReg()\n",
    "\n",
    "model_lr.train(train_feat, train_y)\n",
    "lr_test_pred = model_lr.predict(test_feat)\n",
    "\n",
    "print(f\"Model: {model_type}\", f\", Features: {feature_list}\" if model_type in {\"MLP\", \"LogReg\"} else \"\")\n",
    "if \"word_vector\" in feature_list:\n",
    "    print(f\"Vectorizer: {vect}\") \n",
    "    print(f\"Vector PCA: {vect_pca}\")\n",
    "print(\"Accuracy: \", accuracy_score(test_y, lr_test_pred))\n",
    "print(\"F1 Score: \", f1_score(test_y, lr_test_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity 85.70780399274047%\n"
     ]
    }
   ],
   "source": [
    "same = 0\n",
    "diff = 0 \n",
    "for l, m, sentence in zip(lr_test_pred, mlp_test_pred, test_x):\n",
    "    if l == m:\n",
    "        same += 1\n",
    "    else:\n",
    "        diff += 1\n",
    "\n",
    "print(f\"Similarity {same * 100 / (same + diff)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
